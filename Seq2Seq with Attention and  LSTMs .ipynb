{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19820,"status":"ok","timestamp":1700632014239,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"2Nmxb81cx99S","outputId":"d8f40598-15e6-4328-c4f0-f690b3b156af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"6If-tT1gAA3J"},"source":["### This notebook is best accessed through Google Colab here: https://colab.research.google.com/drive/1IYk6vCo8dk6UgbO2ETEt3LeRPeR3G2Sd?usp=sharing\n","\n","### The folder containing all files needed for this project (including trained models that can be directly loaded) can be downloaded here: https://drive.google.com/drive/folders/1lZVst4fI5_FrjIJPeTh8bxKzltZzJRYX?usp=sharing\n"]},{"cell_type":"markdown","metadata":{"id":"074a0EpQxbKH"},"source":["# Personalized Autocomplete\n","### Next word prediction part 2 - Using sequence to sequence model with attention"]},{"cell_type":"markdown","metadata":{"id":"YQX9t6mqxbKJ"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"7dbctM6DxbKJ"},"source":["The following project is a continuation of the previous project, where we worked on developing an architecture that relied on converting the input text sequences using GloVe embeddings and then developing an architecture that relied on LSTM cells to make predictions for the next sequences.\n","\n","In this project, we will be using a different architecture that relies on sequence to sequence model with attention. The vanilla architecture was proposed in the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) by Bahdanau et al. In this project, we will be taking the proposed architecture and improving upon it to make it suitable for the task of next word prediction.\n","\n","Moreover, in addition to accuracy we used in the last project, we will be including several more sophisticated metrics: *top-k accuracy*, *perplexity*, *cosine similarity*.\n","\n","**Important note:** his notebook *directly* reuses some of the ***functions*** **and** ***explanations*** from the previous notebook; e.g. entirety of preprocessing and data generation functions and their explanations are the same."]},{"cell_type":"markdown","metadata":{"id":"iucboDlOxbKK"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"FRu5GkDnxbKK"},"source":["In the previous project, the original plan of the project was to track all of my keylogs using a keylogger (the code for which is provided below) and using that train a model that would serve as a personalized autocompleter. However, as demonstrated in the previous project, the keylogs were not nearly enough to provide great results. A simple proof of this is that the number of unique tokens (words) was less than 3,500. This is because at the time I had only used the keylogger for a couple of days. Compared to around 47,000 average number of unique tokens that I have used in the last 2-3 months, as provided by Grammarly, this is a very small number that is not enough to provide representative or great results even with an excellent model.\n","\n","To battle this, I had merged the keylogs with several public domain books that I believe use language somewhat similar to mine and that I find have had an important impact on my linguistic style such as Harry Potter book series, some Dostoyevsky works, etc. which were available on project Gutenberg. This increased the number of unique tokens to beyond 100,000 and provided much better results.\n","\n","The number of words needed to provide great results is unfortunately more of an art than a science. It is heavily dependent on the architecture choices,  complexity of the language, the specific domain or genre of the text, and the quality of the data. In general, the more data you have, the better the results will be.\n","\n","For ChatGPT it turned out that \"just\" 175 billion parameters, a slightly more sophisticated transformers architecture and probably between 1 trillion and 100 trillion words proved sufficient to be able to model the complex distribution that human speech has in about 95 languages (as per Mr. Wolfram's [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/))\n","\n","In the case of the model that we will be using in this project, the AI lore suggests that between tens to hundreds of thousands of sentences should be sufficient to provide great results, which is far greater than the number of words that I have logged in the last month.\n","\n","Rather than just adding books data and by doing that dilluting the impact of my own personal data, I have put an emphasis on using my own data. Besides having more of my own personal data, I have scraped all of the text I have written in previous papers, reports, projects, etc. As this still is not comparable to the amount of text in books, I have decided to add a data augmentation step to my pipeline.  \n","\n","I have also ran the model on the exact same dataset as in the previous project for comparison purposes."]},{"cell_type":"markdown","metadata":{"id":"Z7d8ALTQxbKL"},"source":["### The (mathematical) goal of the project\n","\n","*the following text is similar to the one in the previous project*\n","\n","Our goal is remains the same. Given an input of sequence of words, predict what the most likely next word in the sequence is.\n","\n","As before, although our end goal will at times be to generate entire paragraphs form the input text, we will only be predicting a single word *at a time*. Although predicting more words in a batch is certainly possible, single word prediction is useful since it is more close to the autocomplete feature and accuracy is higher given a smaller dataset.\n","\n","Since we want to generate sequences longer than just 1 word, we will predict the next word $w_{i+1}$ and then append the predicted to the previous input sequence. Using that newly updated input sequence we will be able to generate the next word in the sequence. This process can be repeated to generate as long of a sequence as we want.\n","\n","#### Example\n","\n","To demonstrate, let's say that the number of words we predict on is 3 and our initial input sequence is $\\{\"today\", \"was\", \"a\"\\}$ and our goal is to predict the next 2 words.\n","\n","Let us assume that our model predicted the next word in the sequence to be \"good\" giving us the total sequence to be $\\{\"today\", \"was\", \"a\", \"good\"\\}$. We can now move the sliding window of size 3 to use the last 3 words of this (we added \"good\" to the end of the input sequence and dropped \"today\"). We now use this new input sequence $\\{\"was\", \"a\", \"good\"\\}$ to predict the next word which would ideally be $\\{\"day\"\\}$ fulfilling Ice Cube's lyrics. However, it is entirely possible that some other word could be predicted such as $\\{\"boy\"\\}$. This is entirely dependent on the distribution learned from the training data. \"was a good boy\" makes sense, however, \"today was a good boy\" does not. To mitigate this, we will use much larger sequences of words (around 50/100/150/200) to predict the next word in the sequence which will hopefully provide more context and make the model more robust.\n","\n","Essentially, the goal of the model is to learn and mimic from the training data as best as possible the probability distribution of the next word, given the previous words in the sequence, which is done through Bayes' rule:\n","\n","We can represent the abovementioned problem as a conditional probability problem:\n","\n","$P(w_{t+1} | w_1, w_2, ..., w_{t})$\n","\n","where $w_{t+1}$ is the target word we want to predict, and $w_1$, $w_2$, ..., $w_{t}$ are the previous words in the sequence. $w_{t+1}$ can be any of the words in the vocabulary $V$. Vocabulary is the set of all possible words that can be predicted.\n","\n","Our goal is to find the word $w_{t+1}$ that maximizes the probability of the next word given the previous words. To do this, we find need to find the probability distribution for each word in the vocabulary.\n","\n","For example, if we have the vocabulary $V = \\{a, b, c, d\\}$ and the input sequence is $w_1 = a, w_2 = b, w_3 = c$, and we want to predict the next word $w_4$, we want to calculate the probability of each word in the vocabulary being the next word in the sequence.\n","\n","Thus, we want to find:\n","\n","$P(w_4 = a | w_1 = a, w_2 = b, w_3 = c)$ \u003cbr\u003e\n","$P(w_4 = b | w_1 = a, w_2 = b, w_3 = c)$ \u003cbr\u003e\n","$P(w_4 = c | w_1 = a, w_2 = b, w_3 = c)$\n","\n","After obtaining the probability of each word in the vocabulary being the next word in the sequence, we can simply pick the word with the highest probability.\n","\n","\n","To be more mathematically rigorous, we can express this probability through the chain rule of probability by multiplying the probabilities of each word in the sequence:\n","* Probability of the first word in the sequence being $w_1$ = $P(w_1)$\n","* Probability of the second word in the sequence being $w_2$ given that the first word in the sequence is $w_1$ = $P(w_2 | w_1)$\n","* Probability of the third word in the sequence being $w_3$ given that the first two words in the sequence are $w_1$ and $w_2$ = $P(w_3 | w_1, w_2)$\n","* And so on...\n","\n","In general, we have the probability that the word $w_i$ is word $V_{kth}$ which represents the $kth$ word in vocabulary, given that first i-1 words in the sequence are $w_1, w_2, ..., w_{i-1}$ as\n","\n","$ P(w(1), w(2), ..., w(i)=V_{kth}) \\\\=P(w(1)) \\times P(w(2) | w(1)) \\times P(w(3) | w(1), w(2)) \\times ... \\times P(w(i) = V_{kth} | w(1), w(2), ..., w(i-1)) \\\\ =\\Pi_{t=1}^{T} P(w_{t} = V_{kth}| w_{t},...,w_{1})$\n","\n","Now, although uninferrable for us humans through simple rules, our training data (and every text for that matter) has some probability distribution for each word in the vocabulary given some input sequence. Learning and proximatomg this distribution as best as possible is exactly what our model does.\n","\n","Extending this idea further, a bigger and more diverse representative dataset of my own speech, would lead to a far more representative distribution of my own \"true\" speech (if conceptually such a thing exists).\n","\n","Referring back to ChatGPT, since their goal was to somewhat mimic all human knowledge, they needed to use a dataset that had all available human knowledge (books, webpages, video transcriptions, etc.)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Be8ibI3GxbKN"},"source":["## Pipeline"]},{"cell_type":"markdown","metadata":{"id":"FiXXgLedxbKO"},"source":["\n","Below is the general overview of the project pipeline (slight additions to the pipeline from the previous project). Individual choices and decisions are explained in the corresponding sections.\n","\n","1. Get data\n","    1. Keylogs\n","        1. Use keylogger to record keystrokes\n","        2. Merge keylogs into one file\n","    2. Extract all of my own text documents:\n","        1. Research papers\n","        2. Notes\n","        3. Emails\n","        4. Projects\n","        5. Text messages\n","    3. Books and articles from Project Gutenberg\n","        1. Download books and articles\n","        2. Merge books and articles into one file\n","2. Data augmentation of my own personal data\n","    1. Rewrite sentences with synonyms instead of particular words\n","    2. Rewrite sentences with some word exchanged with words that are closest to them in the embedding space.\n","    3. Paraphrase sentences in many different ways using a pretrained LLM (like ChatGPT)\n","    4. Duplicate the text.\n","3. Merge all text into a single file\n","4. Data processing\n","    1. Lowercase\n","    2. Single whitespace\n","    3. Remove punctuation\n","    4. Remove numbers\n","    5. Widen contractions\n","    6. (keylogs) Fix typos\n","    7. (keylogs) Remove emojis\n","    8. [optional] (keylogs) Remove non-English\n","    9. Remove stopwords\n","    9. Split into tokens\n","    10. Lemmatize or Stem (Lemmatize preferred)\n","5. Tokenization\n","6. Create features and target sequences (N-grams)\n","7. Split into train, validation, and test sets\n","    1. Train: 80%\n","    2. Validation: 10%\n","    3. Test: 10%\n","8. Vectorization - Creating an embedding matrix from GloVe\n","9. Create and train the model: Seq2Seq with Attention and LSTM\n","    1. Model architecture\n","    2. Hyperparameters\n","    3. Training\n","    4. Save model\n","10. Evaluation\n","    1. Accuracy\n","    2. Top k accuracy\n","    3. Perplexity\n","    4. Cosine similarity\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ozjgOOxtxbKO"},"source":["Necessary installs:\n","1. pip install TextBlob\n","2. pip install tensorflow (or tensorflow-gpu) and keras\n","3. pip install nltk\n","4. pip install git+https://github.com/MCFreddie777/language-check.git\n","5. pip install contractions\n","6. pip install pycontractions\n","7. pip install numpy\n","8. pip install scikit-learn\n","9. pip install pandas\n","10. pip install matplotlib\n","11. pip install regex\n","12. pip install pynput\n","13. pip install win32gui\n","14. pip install fuzzywuzzy"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27653,"status":"ok","timestamp":1700632041890,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"4aPd_Qd12vJM","outputId":"e16afbea-0aa7-47c9-cdb5-8127d1334e79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch\u003e=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch\u003e=0.0.21-\u003econtractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch\u003e=0.0.21-\u003econtractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"]}],"source":["!pip install contractions # if running in colab this is necessary"]},{"cell_type":"markdown","metadata":{"id":"bcs4zJIoxbKP"},"source":["## Keylogger"]},{"cell_type":"markdown","metadata":{"id":"Tqp3k4t2xbKP"},"source":["\n","Below I have written the code for the keylogger. This is only to demonstrate the code and not to be run (you can click f12 to terminate the script if running). This script can be found withing the './keylogger/' folder. It was run on startup (the code to create it to a bat file and to run it on startup can be found in the './keylogger/' folder as well).\n","\n","The final optimized version of the keylogger found below fully record everything that the user inputs into the keyboard and mimics functions such as ctrl + backspace as well (which I frequently use). One drawback is that it always assume that the cursor pointer is at the end of the text, however, given that this does not happen most of the time which it statistically won't, given enough data, it will not be a problem and can be dealt with in the data processing stage. The keylogger keeps a dictionary for every active window the user writes and then applies the text changes done in that window. When the script terminates, all the values of the dictionary are added sequentially thus preventing the previous problem of words being from mixed contexts. Finally, for every different run the keylogger generates a different txt file.\n","\n","For iOS users, use use pyautogui.getActiveWindowTitle() to get the active window instead of win32gui."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700632041890,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"0aQnpp38xbKQ"},"outputs":[],"source":["def run_keylogger():\n","    import logging\n","    from pynput import keyboard\n","    import os\n","    import pickle as pkl\n","    import re\n","    from win32gui import GetWindowText, GetForegroundWindow\n","\n","    # Log the keystrokes to the console with the current date and time\n","    logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s')\n","\n","    # List of non-alphanumeric characters that are allowed to be typed\n","    SPECIAL_CHARS = ('.', ';', ',', '/', '\\\\', '\"', \"'\", '(', ')',\n","                     '[', ']', '{', '}', '\u003c', '\u003e', '!', '?', ':', '-', '_', '=', '+', '*', '\u0026', '^', '%', '$', '#', '@', '~', '`')\n","    # List of keys to ignore\n","    NONALLOWED_KEYS = [keyboard.Key.esc, keyboard.Key.caps_lock, keyboard.Key.ctrl_r,\n","                       keyboard.Key.alt, keyboard.Key.alt_l, keyboard.Key.alt_r, keyboard.Key.cmd, keyboard.Key.cmd_l,\n","                       keyboard.Key.cmd_r, keyboard.Key.up, keyboard.Key.down, keyboard.Key.left, keyboard.Key.right,\n","                       keyboard.Key.f1, keyboard.Key.f2, keyboard.Key.f3, keyboard.Key.f4, keyboard.Key.f5, keyboard.Key.f6,\n","                       keyboard.Key.f7, keyboard.Key.f8, keyboard.Key.f9, keyboard.Key.f10, keyboard.Key.f11, keyboard.Key.delete]\n","\n","    # deletes the existing pickle file - uncomment only if you want to start from 1 again\n","    #open(\"lastfile.pkl\", \"w\").close()\n","\n","    try:  # try to load the pickle file\n","        with open('lastfile.pkl', 'rb') as fp:\n","            lastfile = pkl.load(fp)  # load the pickle file\n","            logging.info(\n","                f\"Pickle file found. Last log file number: {lastfile}. Initializing log file {lastfile+1}.\")\n","    except (FileNotFoundError, EOFError):  # error catch if the file is not found or empty\n","        logging.info(\"Pickle file not found. Initializing log files from 1.\")\n","        lastfile = 0\n","\n","    # GLOBAL VARIABLES\n","    # Create a dictionary to store the words written in the currently active window\n","    window_words = {}  # key: window title, value: list of words\n","    ctrl_l_pressed = False  # check if ctrl_l is pressed\n","\n","    def mimic_ctrl_backspace(input_string):\n","        \"\"\"\n","        Mimics the ctrl + backspace functionality in the terminal.\n","        Takes the input string, assumes the cursor is at the end of the string and mimics what happens when you press ctrl + backspace.\n","\n","        Parameters\n","        ----------\n","        input_string : str\n","            The string to mimic the ctrl + backspace functionality on.\n","\n","        Returns\n","        -------\n","        str\n","            The modified string after the ctrl + backspace functionality is applied.\n","        \"\"\"\n","        if len(input_string) \u003c= 1:  # if the input string is empty or has only one character\n","            return ''\n","        else:  # len(input_string) \u003e 1\n","            # if the last two characters are spaces\n","            if input_string[-1] == ' ' and input_string[-2] == ' ':\n","                # remove all the whitespace at the end of the input_string\n","                input_string = re.sub(r\"\\s+$\", \"\", input_string)\n","                return input_string\n","            # if the last character is a space and the penultimate character is alphanumeric\n","            elif input_string[-1] == ' ' and input_string[-2].isalnum():\n","                # first remove all the whitespace at the end of the input_string\n","                input_string = input_string.rstrip()\n","                while len(input_string) \u003e 0 and input_string[-1] not in SPECIAL_CHARS and input_string[-1] != ' ':\n","                    # keep removing last character until a space or a special character is found or the string is empty\n","                    input_string = input_string[:-1]\n","                return input_string\n","            else:\n","                if input_string[-1].isalnum():  # if the last character is alphanumeric\n","                    while len(input_string) \u003e 0 and input_string[-1] not in SPECIAL_CHARS and input_string[-1] != ' ':\n","                        # keep removing last character until a space or a special character is found or the string is empty\n","                        input_string = input_string[:-1]\n","                # if the last character is not alphanumeric (i.e. a special character)\n","                else:\n","                    while len(input_string) \u003e 0 and not input_string[-1].isalnum():\n","                        # keep removing last character until an alphanumeric character is found or the string is empty\n","                        input_string = input_string[:-1]\n","                return input_string\n","\n","    def save_keystrokes():\n","        \"\"\" Saves the keystrokes to a file.\n","        The file is saved in the logs folder and is named Log \u003clastfile+1\u003e.txt\n","\n","        Returns\n","        -------\n","        None\n","        \"\"\"\n","\n","        global lastfile\n","        lastfile += 1  # increment the lastfile number\n","\n","        try:\n","            # create the filename\n","            file_name = f\"Log {lastfile}.txt\"\n","            # get the current path\n","            path = os.path.dirname(os.path.realpath(__file__))\n","            # open the file in write mode (creates or overwrites the existing file)\n","            with open(f'{path}/logs/{file_name}', \"w\") as file:\n","                for text in window_words.values():  # iterate through the words written in each active window\n","                    # append the words sequentially so there are mixed sentences from different windows\n","                    file.write(text)\n","                    file.write(\" \")\n","                logging.info(\"Saved keystrokes to file: %s\", file_name)\n","\n","        except Exception as e:\n","            logging.error(\"Could not save keystrokes to file: %s\", str(e))\n","\n","    def on_press(key):\n","        \"\"\"Appends the pressed key to the keystroke string for the currently active window. Updates the dictionary.\n","\n","        Parameters\n","        ----------\n","        key : keyboard.Key\n","            The key that was pressed.\n","\n","        Returns\n","        -------\n","        None\n","        \"\"\"\n","\n","        global ctrl_l_pressed\n","\n","        try:\n","            # get the title of the currently active window\n","            # Works only on Windows. For iOS, use pyautogui.getActiveWindowTitle()\n","            active_window = GetWindowText(GetForegroundWindow())\n","            if active_window not in window_words:  # if the active window has not been used before, initialize it\n","                window_words[active_window] = \"\"\n","\n","            # get the keystroke string for the currently active window\n","            keystroke_log = window_words[active_window]\n","\n","            if key in NONALLOWED_KEYS:  # if the pressed key is not allowed\n","                pass\n","            elif key == keyboard.Key.f12:  # if the pressed key is f12, stop the keylogger and save the keystrokes\n","                listener.stop()\n","                save_keystrokes()\n","                logging.info(\"Stopped the keylogger.\")\n","                pass\n","            # if the pressed key is space or enter, add a space to the keystroke_log\n","            elif key == keyboard.Key.space or key == keyboard.Key.enter:\n","                keystroke_log += \" \"\n","            # checking if ctrl_l and backspace are pressed at the same time\n","            elif key == keyboard.Key.ctrl_l:  # if the pressed key is ctrl_l, set ctrl_l_pressed to True\n","                ctrl_l_pressed = True\n","            elif key == keyboard.Key.backspace:  # if the pressed key is backspace\n","                # if ctrl_l is pressed and the keystroke_log is not empty\n","                if ctrl_l_pressed and len(keystroke_log) \u003e 0:\n","                    # mimic the ctrl + backspace functionality\n","                    keystroke_log = mimic_ctrl_backspace(keystroke_log)\n","                else:  # if ctrl_l is not pressed or the keystroke_log is empty\n","                    # remove the last character from the keystroke_log\n","                    keystroke_log = keystroke_log[:-1]\n","            # if the pressed key is alphanumeric or a special character\n","            elif key.char.isalnum() or key.char in SPECIAL_CHARS:\n","                # append the pressed key to the keystroke_log\n","                keystroke_log += str(key).strip(\"'\")\n","\n","            # log the keystroke_log for the currently active window\n","            logging.info(f\"{active_window}: {keystroke_log}\")\n","            # update the keystroke_log for the currently active window\n","            window_words[active_window] = keystroke_log\n","\n","        except:\n","            logging.error(\n","                f\"Error while appending keystroke {key}. Unallowed character.\")\n","\n","    def on_release(key):\n","        \"\"\"Checks if the released key is ctrl_l and sets ctrl_l_pressed to False.\n","\n","        Parameters\n","        ----------\n","        key : keyboard.Key\n","            The key that was released.\n","\n","        Returns\n","        -------\n","        None\n","        \"\"\"\n","\n","        global ctrl_l_pressed\n","\n","        if key == keyboard.Key.ctrl_l:  # if the released key is ctrl_l, set ctrl_l_pressed to False\n","            ctrl_l_pressed = False\n","\n","    try:\n","        # Start the keyboard listener and the timer to save the keystrokes\n","        with keyboard.Listener(on_press=on_press, on_release=on_release) as listener:\n","            listener.join()\n","    finally:  # save the lastfile number to a file\n","        with open('lastfile.pkl', 'wb') as fp:\n","            pkl.dump(lastfile, fp)\n","            print(\"Saved new last file number: \", lastfile)\n","            # wait for the user to press enter before closing the terminal\n","            input(\"\\nPress Enter to close the terminal...\")\n"]},{"cell_type":"markdown","metadata":{"id":"MLNNPzHyxbKR"},"source":["### Reading the data"]},{"cell_type":"markdown","metadata":{"id":"UFzNciQ9xbKR"},"source":["#### Merging text files / keylogger logs\n","\n","Given that the keylogger has been running for a few days or that there are several books, the txt files have to be merged into a single txt file."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700632041890,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"WsHGFIl1xbKS"},"outputs":[],"source":["def merge_txt_files(input_dir=\".\\keylogger\\logs\", output_dir=\"database/merged\", name=\"master.txt\"):\n","    \"\"\"Merges all .txt files in the input directory into a single .txt file in the output directory.\n","\n","    Parameters\n","    ----------\n","    input_dir : str\n","        The path to the directory containing the .txt files to merge. Defaults to \".\\keylogger\\logs\".\n","    output_dir : str\n","        The path to the directory where the merged 'master.txt' file will be saved. Defaults to \"database\".\n","\n","    Returns\n","    -------\n","    None\n","    \"\"\"\n","\n","    import os\n","\n","    # Create the output directory if it does not exist\n","    if not os.path.exists(output_dir):\n","        print(\n","            \"Creating a folder '{output_dir}' to store the merged text file...\")\n","        os.makedirs(output_dir)\n","\n","    # Merge the contents of all .txt files in the input directory into a single string\n","    merged_text = \"\"\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith(\".txt\"):\n","            with open(os.path.join(input_dir, filename), \"r\") as f:\n","                merged_text += f.read()\n","    print(\n","        f\"Merged all .txt files from the {input_dir} folder into a single variable.\")\n","\n","    # Write the merged text to a new file in the output directory\n","    output_filename = os.path.join(output_dir, name)\n","    with open(output_filename, \"w\") as f:\n","        f.write(merged_text)\n","    print(f\"Saved the merged text to ./{output_filename}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700632041891,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"46pMxVU-WjU5"},"outputs":[],"source":["#merge_txt_files(input_dir='/content/drive/MyDrive/PredictNextWords/database/hp_unprocessed', output_dir='/content/drive/MyDrive/PredictNextWords/database/merged')"]},{"cell_type":"markdown","metadata":{"id":"ilyu3F0ZxbKS"},"source":["#### Reading the master text file\n","\n","Below is the function used to read a text file and return a string containing the entire dataset which we use in our preprocessing."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700632041891,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"cUNOsvoOxbKS"},"outputs":[],"source":["def read_txt_file(file_name, folder_path=\"/content/drive/MyDrive/PredictNextWords/database/processed/\", encoding=\"utf-8\"):\n","    \"\"\"Reads a text file.\n","\n","    Parameters\n","    ----------\n","    file_name : str\n","        The name of the text file.\n","    folder_path : str (optional)\n","        The path to the folder containing the text file. Defaults to \"./database/processed/\".\n","    encoding : str (optional)\n","        The encoding of the text file. Defaults to \"utf-8\".\n","\n","    Returns\n","    -------\n","    text : str\n","        The text read from the file.\n","    \"\"\"\n","\n","    import os  # Import the os module to work with file paths\n","\n","    with open(os.path.join(folder_path, file_name), 'r', encoding=encoding) as f:\n","        text = f.read()\n","\n","    print(\n","        f\"\\nRead {file_name}. It contains {len(text)} characters and {len(text.split())} words.\")\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"q8dacN3mxbKT"},"source":["## Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"NPrtUDlx1IN9"},"source":["\n","Primarily, we will be using two libraries for preprocessing and data augmentation: regex and nltk.\n","\n","Regex is used for simpler text cleaning such as finding and removing punctuation, numbers, etc., whereas NLTK is a very standard library used for more complex tasks such as tokenization, lemmatization, stemming, which can be easily done with its built-in functions."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5921,"status":"ok","timestamp":1700632047807,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"fjfhCsCL1A6M","outputId":"b4d6414e-7a50-4ef0-a91c-193a72b1c3dd"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import re  # regular expressions\n","import nltk  # natural language toolkit\n","nltk.download('popular')  # uncomment to download the nltk data\n"]},{"cell_type":"markdown","metadata":{"id":"SkhdRT9LxbKT"},"source":["As explained above, I have decided to use data augmentation to increase the size of my dataset so it does not get dilluted when combined with the text from other sources (such as books.)\n","\n","There are several methods of data augmentation that can be used and that I have used.\n","1. **Just recopying the same text (duplicating it) multiple times.** \u003cbr\u003e\n","    This is the simplest method and can be used to increase the size of the dataset by a factor of 2, 3, 4, etc. However, this method does not provide any new information (the size of the vocabulary remains the same) and thus does not help the model learn as much variety as other methods below. Interestingly, although not necessarily proven it appears that for GPT models (perhaps also for vanilla transformer), this helps the model solidify what it has learned which is analogous to how we humans tend to remember things better when we repeat them multiple times ([What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)). Testing whether this is also true for seq2seq models is left for future work.\n","\n","    Giving it the benefit of the doubt, I have used this only once by duplicating (once) the text after all the other methods have been applied.\n","1. **Rewriting sentences with a couple of words being replaced with synonyms from a thesaurus.** \u003cbr\u003e\n","    For example: If we have a sentence: \"He was really happy with his new vehicle.\", we can replace the word \"happy\" with a synonym such as \"ecstatic\" and \"vehicle\" with \"car\" to get \"\"He was really happy with his new vehicle.\"\n","2. **Rewriting sentences with a couple of words being replaced with most similar words.** \u003cbr\u003eWe can accomplish this using pretrained embeddings such as GloVe by randomly pick one or two words and for each of them find the closest words in the embedding space and replace the word with them thus getting new sentences with similar meaning but different words. The implementation of the following is seen in the cell below.\n","3. **Paraphrasing sentences in many different ways using a pretraiend LLM (like ChatGPT).**\u003cbr\u003eThis was the primary method that I have used as it was really fast and provided the best results (by my subjective metric)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1700632047808,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"iFJZuWPYxbKT"},"outputs":[],"source":["# SECOND METHOD - USING GLOVE EMBEDDINGS\n","\n","import os\n","import numpy as np\n","from scipy.spatial.distance import cdist\n","from nltk.tokenize import word_tokenize\n","\n","\n","def replace_words_with_closest_glove_words(text, num_sentences=3, root_dir = \"\", glove_dir='glove/glove.6B', embedding_dim=50):\n","    \"\"\"\n","    Takes in an input sentences and replaces 1 or 2 words in a text with their closest words in GloVe embeddings.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The input text. Example: \"He was really happy with his new vehicle.\"\n","    num_sentences : int\n","        The number of sentences to generate. Defaults to 3.\n","\n","    Returns\n","    -------\n","    sentences : list\n","        Modified sentences.\n","        Example: [\"He was really happy with his new vehicle.\",\n","                  \"He was really happy with his new car.\",\n","                  \"He was really happy with his new car.\"]\n","    \"\"\"\n","\n","    # Load GloVe embeddings\n","    glove_path = f\"{root_dir}/{glove_dir}/glove.6B.{embedding_dim}d.txt\"\n","    word_vectors = {}\n","    with open(glove_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.strip().split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            word_vectors[word] = vector\n","\n","    # Tokenize input text\n","    tokens = word_tokenize(text)\n","\n","    # Generate sentences with replaced words\n","    sentences = []\n","    for i in range(num_sentences):\n","        # Choose one or two words to replace\n","        num_words_to_replace = np.random.choice([1, 2])\n","        words_to_replace = np.random.choice(\n","            tokens, size=num_words_to_replace, replace=False)\n","\n","        # Find closest words in GloVe embeddings\n","        closest_words = []\n","        for word in words_to_replace:\n","            if word in word_vectors:\n","                word_vector = word_vectors[word]\n","                distances = cdist(word_vector.reshape(1, -1),\n","                                  list(word_vectors.values()))\n","                closest_word_idx = np.argsort(distances)[0][1]\n","                closest_word = list(word_vectors.keys())[closest_word_idx]\n","                closest_words.append(closest_word)\n","\n","        # Replace words in tokenized text\n","        new_tokens = []\n","        j = 0\n","        for k, token in enumerate(tokens):\n","            if token in words_to_replace:\n","                if j \u003c len(closest_words):\n","                    new_tokens.extend(word_tokenize(closest_words[j]))\n","                    j += 1\n","                else:\n","                    new_tokens.append(token)\n","            else:\n","                new_tokens.append(token)\n","\n","        # Generate sentence with replaced words\n","        new_sentence = ' '.join(new_tokens)\n","        sentences.append(new_sentence)\n","\n","    return sentences\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15733,"status":"ok","timestamp":1700632063531,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"v-jOvZZuxbKT","outputId":"80a2bc64-e91b-4903-8f29-99b9860354a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original text:  He was really happy with his new vehicle.\n","['He was really happy and he new vehicle .', 'He was really happy with he for vehicle .', 'He was really happy with he new vehicle .']\n"]}],"source":["test_string = \"He was really happy with his new vehicle.\"\n","print(\"Original text: \", test_string)\n","print(replace_words_with_closest_glove_words(test_string, root_dir = \"/content/drive/MyDrive/PredictNextWords\"))\n"]},{"cell_type":"markdown","metadata":{"id":"apRD1paTxbKU"},"source":["## Preprocessing\n","\n","***This section is (almost) the same as in the previous project***"]},{"cell_type":"markdown","metadata":{"id":"8C6N1X0fxbKV"},"source":["\n","\n","Preprocessing is the fundamental process of preparing the data for the model. It is a very important step as it can have a significant impact on the model's performance (can reduce complexity and number of features, increase performance by finding only the key features, etc.)\n"]},{"cell_type":"markdown","metadata":{"id":"aNKkkBcFxbKV"},"source":["#### Converting to lowercase and stripping multiple whitespaces."]},{"cell_type":"markdown","metadata":{"id":"YKhHWBwhxbKV"},"source":["Converting all text to lowercase makes the vocabulary size smaller and prevents the model from learning different embeddings for the same word in different cases. For example, without lowercasing, the model would learn different embeddings for \"Apple\" and \"apple,\" even though they refer to the same thing. If we want to do capitalization, it is better to do train the model with all lowercase so the model produces better results and then apply capitalization in the post-processing stage by applying some model that capitalizes.\n","\n","The texts used contain a lot of whitespaces and although they will not necessarily increase the vocabulary size (a tokenizer will easily get rid of them), looking at the text, they are not necessary and take away from readability."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1700632063531,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"BO0czh1dxbKW"},"outputs":[],"source":["def lowercase_and_strip_whitespaces(text):\n","    \"\"\"Converts a given text to lowercase and strips multiple whitespaces to a single whitespace.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to convert.\n","\n","    Returns\n","    -------\n","    str\n","        The converted text with multiple whitespaces stripped to a single whitespace and converted to lowercase.\n","    \"\"\"\n","    # strip multiple whitespaces\n","    text = ' '.join(text.split())\n","\n","    # convert to lower case\n","    text = text.lower()\n","\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"0elF94y2xbKW"},"source":["#### Remove punctuation"]},{"cell_type":"markdown","metadata":{"id":"-YFHgAIsxbKX"},"source":["Analogous to above, punctuation marks and numbers typically do not carry much meaning in natural language and can be safely removed without significant loss of information. Removing punctuation and numbers can also help to reduce the noise in the data, since these characters can appear in many different contexts and make it more difficult for models to learn the correct relationships between words.\n","\n","In the case of next word prediction, removing punctuation and numbers can also help to ensure that the model is able to focus on the words themselves rather than being distracted by other characters in the input. This can help to improve the accuracy and effectiveness of the model in predicting the most likely next word given a sequence of words."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1700632063531,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"lQgkKpaCxbKX"},"outputs":[],"source":["def remove_punctuation_and_numbers(text):\n","    \"\"\"Removes punctuation and numbers from a given text.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to remove punctuation and numbers from.\n","\n","    Returns\n","    -------\n","    str\n","        The text with punctuation and numbers removed.\n","    \"\"\"\n","\n","    text = ''.join([char for char in text if char.isalpha() or char == ' '])\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"b_v-tl7HxbKY"},"source":["#### Widen contractions"]},{"cell_type":"markdown","metadata":{"id":"Xg8M7c-PxbKY"},"source":["Another thing we can do is widen the contractions we have in the text. This is useful because it helps to standardize the text and reduce the dimensionality of the data. E.g. we want \"I'm\" and \"I am\" to be deemed the same by the model and we want to reduce the number of unique words that the model needs to learn in order to make accurate predictions.\n","\n","Widening contractions involves expanding commonly used contractions (e.g., \"can't\" to \"cannot\", \"won't\", etc.) into their full forms. By expanding contractions, we are converting a single word (the contraction) into multiple words (the full form), which can help reduce the dimensionality of the data and ensure that the model learns the relationships between the full words rather than just the contractions. This can help improve the model's accuracy and make it more robust to different variations of contractions.\n","\n","Although I have placed this step after the removal of punctuation since it is slightly more complex, we do it before otherwise the punctuation from the contractions will be removed as well making it impossible to widen the contractions.\n","\n","In the code below, I made it possible to use two models to expand contractions: 'contractions' and 'pycontractions'. The first one is a simple model that expands contractions by looking at a dictionary of contractions and their full forms.\n","\n","The 'contractions' library uses a simple dictionary-based approach for expanding contractions. It contains a dictionary of common English contractions and their expanded forms, which is used to replace the contractions in the text. For example, the contraction \"won't\" is expanded to \"will not\". The library is very lightweight and easy to use, but it can't handle some less common or informal contractions that can be expanded to multiple different words (such as \"ain't\"), and it may not work well on text with spelling or grammatical errors. It also does not take context into account reducing its accuracy.\n","\n","The 'pycontractions' library, on the other hand, uses a more sophisticated approach by leveraging a pre-trained language model to identify and expand contractions in text. The library uses spaCy, which is a popular open-source NLP library, to parse the input text and identify contractions. Then, it uses a pre-trained machine learning model that you have to load (such as Glove-twitter-100). The transformer model is fine-tuned on a large corpus of English text to learn the mapping between contractions and their expanded forms which leads to better accuracy and awareness of context - which leads to much better results.\n","\n","The advantage of this approach is that it can handle a wide range of contractions, including those that are less common or informal, and it can also handle variations in spelling or grammar. However, its higher accuracy comes at the price of computational cost and speed. This can be slow for large texts or in real-time applications.\n","\n","Despite making both accessible below, I prioritized using the simple 'contractions' as it is sufficient for this project."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17972,"status":"ok","timestamp":1700632081483,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"CUOH76lcCynX","outputId":"93873619-7972-45d3-846f-c90f86d5a834"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch\u003e=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch\u003e=0.0.21-\u003econtractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch\u003e=0.0.21-\u003econtractions) (2.0.0)\n"]}],"source":["!pip install contractions"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"DGAUcBPExbKY"},"outputs":[],"source":["def expand_contractions(text, model='contractions'):\n","    \"\"\"\n","    Expands contractions in a given text.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to expand contractions in.\n","    model : str\n","        The model to use to expand contractions. Defaults to 'contractions'.\n","        Options:\n","            'contractions' - uses the contractions library\n","            'pycontractions' - uses the pycontractions library. Greater accuracy as it looks at the context of the word. Has frequent dependency issues.\n","\n","    Returns\n","    -------\n","    str\n","        The text with contractions expanded.\n","    \"\"\"\n","\n","    try:\n","        if model == 'contractions':\n","            import contractions\n","            text = contractions.fix(text)\n","            return text\n","\n","        elif model == 'pycontractions':\n","            import pycontractions\n","\n","            # Load the contraction model\n","            cont = pycontractions.Contractions(api_key=\"glove-twitter-100\")\n","            cont.load_models()\n","\n","            # Expand contractions in the text\n","            expanded_text = list(cont.expand_texts([text], precise=True))[0]\n","            return expanded_text\n","        else:\n","            raise Exception(\n","                f\"Model '{model}' is not supported. Please choose either 'contractions' or 'pycontractions'.\")\n","    except Exception as e:\n","        print(f\"Error expanding contractions: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"TfsZwvf8xbKY"},"source":["#### Remove stopwords\n","\n","Removing stopwords such as \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"at\", etc. is a common step in text preprocessing. They are usually removed from text data during preprocessing because they do not add much meaning to the text and are unlikely to be useful for NLP tasks. By removing this, we can sometimes significantly reduce the number of features and improve the performance of the model since only the most important words are kept and used for decision making and the feature space is reduced.\n","\n","Removing stopwords also reduces noise since stopwords are occur very frequently in the corpus of text thereby adding noise to text data. By removing them, we can reduce the number of meaningless words in the text and focus on more meaningful words.\n","\n","Of course, sometimes stopwords can be useful for certain tasks such as matching query items. Thus, treating this as a hyperparameter and comparing the results with and without is always a good idea. In my case, the results were better without stopwords."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"NdZo93jjxbKZ"},"outputs":[],"source":["def eliminate_stopwords(tokens):\n","    \"\"\"Removes stopwords from a given list of tokens.\n","\n","    Parameters\n","    ----------\n","    tokens : list\n","        A list of tokens to remove stopwords from.\n","\n","    Returns\n","    -------\n","    filtered_words_tokens: list\n","        A list of tokens with stopwords removed.\n","\n","    \"\"\"\n","\n","    stopwords = nltk.corpus.stopwords.words(\"english\")\n","\n","    from string import punctuation\n","    # since our text doesn't have punctuation we also remove punctuation from stop words\n","    stopwords = [word for word in stopwords if word not in punctuation]\n","\n","    # remove stop words from tokens\n","    filtered_words = [word for word in tokens if word not in stopwords]\n","\n","    return filtered_words\n"]},{"cell_type":"markdown","metadata":{"id":"CFDTEvuPxbKZ"},"source":["#### Remove emojis"]},{"cell_type":"markdown","metadata":{"id":"U-nwwLJexbKZ"},"source":["There were a couple of emojis in my text so I decided to include a function to remove them. This is oftentimes not necessary but I wanted to include it for completeness. When creating the keylogger, I made sure they are not included and that I do as little preprocessing as possible."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"C_npbffDxbKZ"},"outputs":[],"source":["def remove_emojis(text):\n","    \"\"\"Removes emojis from a given text.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to remove emojis from.\n","\n","    Returns\n","    -------\n","    text : str\n","        The text with emojis removed.\n","    \"\"\"\n","\n","    # remove emojis from the text\n","    emoji = re.compile(\"[\"\n","                       u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n","                       u\"\\U0001F300-\\U0001F5FF\"  # symbols \u0026 pictographs\n","                       u\"\\U0001F680-\\U0001F6FF\"  # transport \u0026 map symbols\n","                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                       u\"\\U00002702-\\U000027B0\"\n","                       u\"\\U000024C2-\\U0001F251\"\n","                       \"]+\", flags=re.UNICODE)\n","    text = emoji.sub(r'', text)\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"1P7gxN4nxbKZ"},"source":["#### Tokenization"]},{"cell_type":"markdown","metadata":{"id":"I81J51sjxbKa"},"source":["Tokenization is a fundamental step in NLP. It takes a string of text (such as \"today is a good day\") and splits it into a list of tokens / word ([\"today\", \"is\", \"a\", \"good\", \"day\"]).  \n","\n","Doing this allows us to analyze text by breaking it down into smaller, more manageable units. It is a critical step in creating a bag-of-words model, which is a simple but effective way to represent text data numerically for use in machine learning algorithms. Generally, it makes it much easier to vectorize text data, to perform other tasks such as counting the number of occurrences of each word in the text, cleaning tasks, such as removing stopwords, stemming or lemmatization, and other preprocessing tasks.\n","\n","Although Tokenization can also help in visualizing the text data. For instance, word clouds can be generated, which can help in quickly identifying the most commonly occurring words in the text. I chose not do this as it did not give any useful insights.\n","\n","NLTK's word_tokenize function is a popular tokenization method that works well for most use cases. It uses regular expressions to split the text into individual words. The function also has built-in features for handling contractions, hyphenated words, and other special cases."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"EW0WackGxbKa"},"outputs":[],"source":["def tokenize(text):\n","    \"\"\"Tokenizes a given text into a list of words.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to tokenize. E.g. \"This is a sentence\"\n","\n","    Returns\n","    -------\n","    list\n","        A list of words. E.g. [\"This\", \"is\", \"a\", \"sentence\"]\n","    \"\"\"\n","\n","    # split text into tokens (words) - also gets rid of multiple whitespaces\n","    tokens = nltk.word_tokenize(text)\n","    return tokens\n"]},{"cell_type":"markdown","metadata":{"id":"3vMHhzhwxbKa"},"source":["#### Correct typos\n","\n","Although this step is not particularly useful for books since they are usually written by professionals and do not have any typos, it is a very useful step for text data that is collected from the internet or social media - or in my case, my personal writting.\n","\n","I have included two libraries: TextBlob and FuzzyWuzzy. The foundation of all of these libraries is the Levenshtein distance which is a string metric for measuring the difference between two sequences. In other words. it is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. The lower the Levenshtein distance between two words, the more similar they are. Thus, intuitively, we change the mispelled word with the word from the dictionary with the smallest Levenshtein distance. However, sometimes there are more of them so using different models can help.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Is8uYDIyxbKa"},"source":["##### TextBlob"]},{"cell_type":"markdown","metadata":{"id":"HNNvV4SQxbKa"},"source":["TextBlob's correct() method has a dictionary-based approach where it compares the misspelled (every word actually) word to its dictionary of known words (it has around 30k English words) and finding the closest match based on its edit distance metric which is a slightly more complicated version of Levensthein distance.\n","\n","The method relies on Bayes' theorem as it uses a probabilistic language model to estimate the likelihood of each possible correction based on the frequency of occurrence of the corrected word in a large corpus of text. This approach helps to correct typos that are not in the dictionary, such as those caused by keyboard errors or phonetic confusion. Its model also takes into account the context of the word and the surrounding words when making corrections. It accomplishes this using a Markov chain model to estimate the probability of each possible correction based on the context of the word and the surrounding words in the text.\n","\n","In a first-order Markov chain model, the probability of a word is dependent only on the previous word in the sequence, and not on any other words that came before it. To estimate the probability of each possible correction, TextBlob's correct() method uses the Bayes' rule formula:\n","\n","$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n","\n","Bayes' rule is a formula that describes how to calculate the probability of an event based on prior knowledge or evidence. In the context of spelling correction, Bayes' rule can be used to calculate the probability of a correction given the context of the word and the surrounding words in the text. Thus,\n","A represents the possible correction for a misspelled word and B represents the context of the word and the surrounding words in the text.\n","\n","This approach helps to correct words that are spelled correctly but are used in the wrong context, such as homophones (e.g., \"there\" vs. \"their\") or words with multiple meanings (e.g., \"bank\" as a financial institution or the side of a river). However, it also sometimes does not correct certain words or wrongly corrects the wrong ones. Finally, it is also statistically possible that some words are wrongly corrected (e.g. 'I love to teech' gets converted to 'I love to teeth')."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"KwPb2WJpxbKb"},"outputs":[],"source":["def correct_typos_textblob(text):\n","    \"\"\"Corrects typoes in a given text using the TextBlob library.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to correct typos in.\n","\n","    Returns\n","    -------\n","    str\n","        The text with typos corrected.\n","    \"\"\"\n","\n","    from textblob import TextBlob\n","    blob = TextBlob(text)\n","    corrected_text = blob.correct()\n","    text = corrected_text.string\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"bdPhZL9VxbKb"},"source":["##### Fuzzy matching - removing typos and non-english words\n","\n","The FuzzyWuzzy library is a Python library that uses Levenshtein distance and other techniques to calculate the differences between sequences and to find matches between strings. The reason I included this is because we can write an algorithm that can potentially not only correct typos but eliminate non-english words as well (which my keylogs had).\n","\n","The 'fuzz.ratio()' scorer calculates a score between 0 and 100 that represents the similarity between two strings based on the number of matching characters they share.\n","\n","In the function below, each word in the input text is compared to a corpus of words using the 'process.extractOne()' function from 'fuzzywuzzy'. This function returns the closest match to the input word in the corpus, along with a score indicating the similarity between the two strings.\n","\n","The fuzz.ratio() scorer used by process.extractOne() calculates the similarity score based on the number of characters that the two strings have in common, as a percentage of the total number of characters in the longer string. For example, if the input word is \"apple\" and the closest match in the corpus is \"apples\", the similarity score would be 83, because \"apple\" and \"apples\" share 5 out of 6 characters, or 83% similarity. If the score is above the threshold, the closest match is considered a valid correction for the input word and is added to the list of corrected words. If the score is below the threshold, no correction is made for that word.\n","\n","This approach is not perfect and can sometimes return the wrong correction. However, assuming that non-english words are going to have a lower similarity score to the English dicitionary, we can use this to eliminate non-english words. In my case, I set the threshold to 80 and it worked well. The problem with this approach is that it can take a very long time to run and of course, there are certain Serbian words that are quite similar to English words or are short enough to be considered as typos."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"QyEgUFjnxbKb"},"outputs":[],"source":["def correct_typos_fuzzy_match(text):\n","    \"\"\"Fuzzy matches a given text to a corpus of words. Returns the closest match if the match is above a certain threshold.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to fuzzy match.\n","    corpus : list\n","        A list of words to use as a reference for fuzzy matching the text.\n","\n","    Returns\n","    -------\n","    str\n","        The fuzzy matched text.\n","    \"\"\"\n","    from fuzzywuzzy import process, fuzz\n","    from nltk.corpus import words\n","\n","    corpus = words.words()\n","\n","    # Split the text into a list of words\n","    words = text.split()\n","\n","    # Correct each word in the text\n","    fuzzy_matched_words = []\n","    for word in words:\n","        # Find the closest match to the word in the corpus\n","        closest_match = process.extractOne(word, corpus, scorer=fuzz.ratio)\n","        # If the closest match is a perfect match, use it\n","        if closest_match[1] \u003e 85:\n","            fuzzy_matched_words.append(closest_match[0])\n","\n","    # Join the corrected words into a string\n","    fuzzy_matched_text = \" \".join(fuzzy_matched_words)\n","    return fuzzy_matched_text\n"]},{"cell_type":"markdown","metadata":{"id":"_pUVhpOtxbKb"},"source":["#### Lemmatization\n","\n","Lemmatization and stemming are techniques used to reduce words to their base/root/lemma form. The difference between them is that stemming simply removes the suffix of a word to derive the root form, while lemmatization maps a word to its base or dictionary form, called a lemma.\n","\n","Stemming is a simpler and more computationally efficient process than lemmatization, but it is also less accurate. Oftentimes this will be sufficient. E.g. 'studies' to 'studi'; 'studying' to 'study'. Other times, for example, a stemming algorithm might convert the word \"running\" to \"run\", but it might also convert the word \"runner\" to \"run\", even though these words have a completely different meaning (one is a verb one is a noun) the meaning of the word is lost. More general drawback of stemming is that for a word such as 'saw', it would most likely always return 'saw' whereas lemmatization is more sophisticated and could potentially return 'see'.\n","\n","Lemmatization is more complex and computationally expensive than stemming and is thus slower for large datasets than stemming. It is generally more accurate because it takes into account the context of the word and its part of speech. The process of lemmatization typically involves identifying the part of speech of a word in a sentence, such as whether it is a noun, verb, adjective, or adverb, and then mapping the word to its corresponding lemma in a predefined vocabulary, such as WordNet. Here, the WordNetLemmatizer uses a set of rules and a pre-defined vocabulary called WordNet to determine the base form of a word based on its part of speech. Lemmatized form of 'mice' would be 'mouse', 'geese' would be 'goose' and 'saw' in a sentence like \"I saw a hourse\" would be 'see', rather than 'saw'.\n","\n","In this case, since the dataset is considered small, lemmatization is much preferred to retain as much information as possible.\n","\n","A lemmatization algorithm, due to its complexity, is much more difficult to create for other languages compared to stemming since it necessitates a deep understanding of the language and its grammatical structural form.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"L2mHv6R0xbKc"},"outputs":[],"source":["def lemmatize_words(tokens):\n","    \"\"\"Lemmatizes a list of words.\n","\n","    Parameters\n","    ----------\n","    tokens : list\n","        A list of words to lemmatize.\n","\n","    Returns\n","    -------\n","    lemmatized_words : list\n","        A list of lemmatized words.\n","    \"\"\"\n","\n","    from nltk.stem import WordNetLemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n","    return lemmatized_words\n"]},{"cell_type":"markdown","metadata":{"id":"FsCG18HIxbKc"},"source":["#### Preprocessing function\n","\n","We can now combine all the preprocessing steps into a single function that can be applied to the entire dataset and easily reused. I also added optional parameters so that certain preprocessing steps can be skipped if not necessary and that verbose is made optional."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1700632081484,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"YgJ3FAQ0xbKc"},"outputs":[],"source":["def data_preprocessing(text, save_file_name=False, root_dir = \"\", verbose=False, remove_stopwords=True, lemmatize=True,  to_remove_emojis=False, to_correct_typos=False, expand_contractions_model='contractions'):\n","    \"\"\"\n","    Preprocesses the text data by: converting to lowercase, removing punctuation,\n","                                      splitting into tokens, removing stop words, and lemmatizing.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to preprocess.\n","    remove_stopwords : bool, optional\n","        Whether to remove stopwords from the text, by default True\n","    lemmatize : bool, optional\n","        Whether to lemmatize the text, by default True\n","    remove_emojis : bool, optional\n","        Whether to remove emojis from the text, by default False\n","    correct_typos : var, optional\n","        Whether to correct typos in the text, by default False. Extremelly slow, use only if absolutely necessary.\n","        You can pass 'textblob' or 'fuzzy_match' to correct typos using the TextBlob library or fuzzy matching, respectively.\n","    expand_contractions_model : var, optional\n","        Model used to expand contractions, by default 'contractions'.\n","        You can pass 'contractions' or 'pycontractions' to expand contractions using the contractions library or nltk, respectively.\n","    save_file_name : string or False, optional\n","        Variable to save the preprocessed text to a file, by default False.\n","        If you want to save the file, pass the file name as a string.\n","\n","\n","    Returns\n","    -------\n","    list\n","        A list of preprocessed tokens.\n","    \"\"\"\n","    if verbose:\n","        print('\\nPreprocessing text...')\n","\n","    # convert to lowercase and strip multiple whitespaces\n","    text = lowercase_and_strip_whitespaces(text)\n","    if verbose:\n","        print('\\tConverted to lowercase and stripped multiple whitespaces.')\n","\n","    # remove punctuation and numbers\n","    text = remove_punctuation_and_numbers(text)\n","    if verbose:\n","        print('\\tRemoved punctuation and numbers.')\n","\n","    # remove emojis\n","    if to_remove_emojis:\n","        text = remove_emojis(text)\n","        if verbose:\n","            print('\\tRemoved emojis.')\n","\n","    # correct typos\n","    if to_correct_typos is not False:\n","        if verbose:\n","            print('\\tCorrecting typos... This may take a while...')\n","        if to_correct_typos == 'textblob':\n","            text = correct_typos_textblob(text)\n","        elif to_correct_typos == 'fuzzy_match':\n","            text = correct_typos_fuzzy_match(text)\n","        if verbose:\n","            print(f'\\tTypos corrected using {to_correct_typos}.')\n","\n","    # expand contractions\n","    if verbose:\n","        print(\n","            f'\\tExpanding contractions using {expand_contractions_model} model...')\n","    text = expand_contractions(text, model=expand_contractions_model)\n","\n","    # tokenize\n","    tokens = tokenize(text)\n","    if verbose:\n","        print('\\tSplit the text into tokens.')\n","\n","    if remove_stopwords:\n","        tokens = eliminate_stopwords(tokens)\n","        if verbose:\n","            print('\\tRemoved stopwords.')\n","\n","    if lemmatize:\n","        tokens = lemmatize_words(tokens)\n","        if verbose:\n","            print('\\tLemmatized words.')\n","\n","    # save preprocessed text to file\n","    if save_file_name is not False:\n","        with open(f\"{root_dir}/database/processed/{save_file_name}\", 'w') as f:\n","            f.write(' '.join(tokens))\n","            if verbose:\n","                print(f'\\tPreprocessed text saved to {root_dir}/database/processed/{save_file_name}')\n","    if verbose:\n","        print(f'Preprocessing finished. There are now {len(tokens)} tokens.\\n')\n","\n","    return tokens\n"]},{"cell_type":"markdown","metadata":{"id":"umSfvDw3xbKc"},"source":["## Tokenization: word to index mappings\n","***This section is (almost) the same as in the previous project***"]},{"cell_type":"markdown","metadata":{"id":"O-gQRnhxxbKd"},"source":["Similar to tokenization explained above. The function 'get_word_to_index_mappings()' takes a list of tokens and creates word-to-index and index-to-word mappings using the Keras Tokenizer class. The tokenizer first fits on the text data, meaning it learns the vocabulary (vocabulary is the list of all unique words found in the corpus of text) of the text and assigns a unique integer index to each word. The word-to-index dictionary contains a mapping from each word in the vocabulary to its unique corresponding integer index. The index-to-word dictionary contains the reverse mapping, from each integer index to its corresponding word.\n","\n","The function also uses the tokenizer to convert the original text to a list of tokens, where each token is represented by its corresponding integer index. This is useful for vectorization and feeding the data to our model later on."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1700632081485,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"BMDDYQRCxbKd"},"outputs":[],"source":["def get_word_to_index_mappings(tokens, root_dir=\"\"):\n","    \"\"\"Creates word-to-index and index-to-word mappings.\n","\n","    Parameters\n","    ----------\n","    tokens : list\n","        A list of tokens. E.g. ['the', 'cat', 'sat', 'on', 'the', 'mat']\n","    root_dir : str, optional\n","        Root directory to save the mappings to, by default \"\"\n","\n","    Returns\n","    -------\n","    word_index : dict\n","        A dictionary with word-to-index mappings. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n","    index_word : dict\n","        A dictionary with index-to-word mappings. E.g. {1: 'the', 2: 'cat', 3: 'sat', 4: 'on', 5: 'mat'}\n","    text_as_tokens : list\n","        A list of tokens. [1, 2, 3, 4, 1, 5]\n","    \"\"\"\n","\n","    from keras.preprocessing.text import Tokenizer\n","    from pickle import dump\n","\n","    # create a tokenizer object\n","    # lower=True converts all text to lowercase, oov_token='\u003cOOV\u003e' replaces all out-of-vocabulary words with \u003cOOV\u003e\n","    tokenizer = Tokenizer(lower=True, oov_token='\u003cOOV\u003e')\n","\n","    # fit the tokenizer on the text data\n","    tokenizer.fit_on_texts(tokens)\n","\n","    # get the word-to-index mappings\n","    word_index = tokenizer.word_index\n","\n","    print(\n","        f'Created word-to-index dictionary. Total number of unique tokens: {len(word_index)}.')\n","\n","    # get the index-to-word mappings\n","    index_word = {v: k for k, v in word_index.items()}\n","\n","    # convert the text to a list of tokens\n","    # the output is a list of lists, so we take the first element\n","    text_tokenized = tokenizer.texts_to_sequences([tokens])[0]\n","\n","    # save the tokenizer object\n","    dump(tokenizer, open(f'{root_dir}/tokenizers_saved/tokenizer.pkl', 'wb'))\n","\n","    return word_index, index_word, text_tokenized\n"]},{"cell_type":"markdown","metadata":{"id":"yIooLdoJxbKd"},"source":["## Split into features and targets: N-grams\n","***This section is (almost) the same as in the previous project***"]},{"cell_type":"markdown","metadata":{"id":"JKqG7WAPxbKd"},"source":["We can now split the dataset into features and targets by appling a rolling window that will extract a sequence of size n. The features are the sequences of words [:-1] and the targets are the final words in the sequences. The target is the word that we want to predict given the sequence of words that precede it. This makes the process a supervised learning problem.\n","\n","These continous overlapping sequences of n words or characters are called N-grams. They can be of varying length but in this case, after trying several different variants I defaulted to sequences of length 50, where the features are the first 49 and the target is the 50th number.\n","\n","We create a window of size N which we move along the sequence of words (our corpus). The window is the sequence of words that we use to predict the next word. The target is the next word in the sequence. Every time we move the window we get a new datapoint.\n","\n","Below we have an example of a Bigram (N=2).\n","\n","![Ngrams example](https://drive.google.com/uc?export=view\u0026id=1nk45KGtdcXGThilBkoBz4wAgLUylPiKd)"]},{"cell_type":"markdown","metadata":{"id":"mD9sxQsvxbKe"},"source":["#### One-hot encoding, features and targets and examples"]},{"cell_type":"markdown","metadata":{"id":"ENWQlLntxbKe"},"source":["\n","Since, in this case, we use N-1 tokens as the features and only 1 as the target, we want to represent that target in a format that is suitable for our Machine Learning model.\n","\n","The target is a single word, but our model needs to be able to predict a probability distribution over the entire vocabulary. In other word and as explained in the introduction of the paper, each of the words $(w_1, w_V)$ will have a conditional probability of being the next word given the input sequence. Thus, since we want our target to be written in the shape of a probability distribution. The target should be a vector of size V, where V is the size of the vocabulary.\n","\n","This can most easily be obtained through one-hot encoding. One-hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n","\n","If our indexed vocabulary is:\n","{0: 'the', 1: 'cat', 2: 'sat', 3: 'on', 4: 'mat'}\n","\n","and our training example is:\n","['the', 'cat', 'sat', 'on', 'the', 'mat'] = [0, 1, 2, 3, 0, 4]\n","\n","Then, the features would be:\n","['the', 'cat', 'sat', 'on', 'the'] = [0, 1, 2, 3, 0]\n","the target would be:\n","['mat'] = [4]\n","\n","Then the one-hot encoded vector for the word 'mat' would be:\n","[0, 0, 0, 0, 1]\n","\n","As we can see, the dimensionality is equal to the size of the vocabulary and everything besides the index of the target word is 0, with the index of the target word being 1.\n","\n","The language model can then use this one-hot encoded target vectors to compute a probability distribution over the entire vocabulary. This is typically done using a softmax activation function on the output layer of the neural network which maps the output of the neural network to a probability distribution over the vocabulary by exponentiating each output and normalizing the sum of the exponentiated values to 1. This gives the probability distribution over the vocabulary and allows us to choose the word with the highest probability as the predicted next word. In training, this probability distribution is also used to compute the loss and update the model parameters during training."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":526,"status":"ok","timestamp":1700632081986,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"qzJexKYNiPfU"},"outputs":[],"source":["def get_features_targets(text_tokenized, total_unique_tokens, one_hot_encoding=True, seq_len=30):\n","    \"\"\"Creates features and targets from a list of tokens.\n","\n","    Parameters\n","    ----------\n","    text_tokenized : list\n","        A list of tokens. E.g. [1, 2, 3, 4, 1, 5]\n","    total_unique_tokens : int\n","        Total number of unique tokens. E.g. 5\n","    one_hot_encoding : bool, optional\n","        Whether to use one-hot encoding for the targets, by default True\n","    seq_len : int, optional\n","        Length of the sequence, by default 30\n","\n","    Returns\n","    -------\n","    features : list\n","        A list of feature sequences. E.g. [[1, 2, 3, 4, 1], [2, 3, 4, 1, 5]]\n","    targets : list\n","        A list of target words. E.g. [5, 1] if not using one-hot encoding, or a list of one-hot encoded vectors if using one-hot encoding.\n","    \"\"\"\n","\n","    from keras.utils import to_categorical  # for one-hot encoding\n","\n","    features = []\n","    targets = []\n","\n","    for i in range(seq_len, len(text_tokenized)):\n","        seq = text_tokenized[i-seq_len:i]\n","        target = text_tokenized[i]\n","        features.append(seq)\n","        if one_hot_encoding:\n","            target_one_hot = to_categorical(\n","                target, num_classes=total_unique_tokens)\n","            targets.append(target_one_hot)\n","        else:\n","            targets.append(target)\n","\n","    if len(features) != len(targets):\n","        raise ValueError(\n","            f'Number of feature examples ({len(features)}) is different from number of targets ({len(targets)}).')\n","\n","    print(\n","        f'Created feature ({len(features[0])} words) and target (1 word) pairs. Total number of datapoints: {len(features)}.')\n","    return features, targets\n"]},{"cell_type":"markdown","metadata":{"id":"e7fzbWrYxbKe"},"source":["## Split into training testing and validation\n","***This section is the same as in the previous project***\n"]},{"cell_type":"markdown","metadata":{"id":"1uYRpY1bxbKf"},"source":["\n","We split the features and targets we obtained above into training, testing and validation sets. The training set is used to train the model. The validation set is used to tune the hyperparameters of the model by trying models with different hyperparameters and then choosing the model that gave the best performance on the validation set. The testing set is used to evaluate the final model on unseen data."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700632081986,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"5WW9tu8OxbKf"},"outputs":[],"source":["def split_training_testing_validation(features, targets, test_size=0.1, val_size=0.1):\n","    \"\"\"\n","    Splits the text into training, testing, and validation sets.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        The text to split.\n","    test_size : float, optional\n","        The size of the testing set, by default 0.1\n","    val_size : float, optional\n","        The size of the validation set, by default 0.1\n","\n","    Returns\n","    -------\n","    X_train : list\n","        A list of training features.\n","    X_test : list\n","        A list of testing features.\n","    X_val : list\n","        A list of validation features.\n","    y_train : list\n","        A list of training targets.\n","    y_test : list\n","        A list of testing targets.\n","    y_val : list\n","        A list of validation targets.\n","    \"\"\"\n","\n","    from sklearn.model_selection import train_test_split\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        features, targets, test_size=test_size+val_size, random_state=56, shuffle=True)\n","    X_test, X_val, y_test, y_val = train_test_split(\n","        X_test, y_test, test_size=val_size/(test_size+val_size), random_state=56, shuffle=True)\n","\n","    print(\n","        f\"Split dataset into training ({(1-test_size-val_size)*100}%), validation ({val_size*100}%), testing({test_size*100}%). Sizes: X_train: {len(X_train)}, X_test: {len(X_test)}, X_val: {len(X_val)}\")\n","\n","    return X_train, X_test, X_val, y_train, y_test, y_val\n"]},{"cell_type":"markdown","metadata":{"id":"pHgUxSr-xbKf"},"source":["## Vectorization\n","\n","***This section is (almost) the same as in the previous project***"]},{"cell_type":"markdown","metadata":{"id":"gtu8RnRHxbKf"},"source":["Before we feed the sequences to our model we want to ensure our numerical representation contains as much useful information as possible. As humansn, we subconsciously recognize whether words are nouns, verbs, adjectives, etc. We understand grammar, context, synonyms, similar words and many other linguistic nuances. Ideally, we want our model to be able to do the same which is not a trivial task considering computers are only able to understand numbers.\n","\n","There are many ways to extract text features such as bag-of-words, TF-IDF, etc. In this case, I will focus on one of the most effective and widely used methods, word embeddings.\n","\n","Most famous word embedding models are Word2Vec, GloVe and FastText. Due to space constraints I will only focus on GloVe which is an extension to Word2Vec.\n","\n","Word embedding is a way of numerically representing words as vectors so that the words with similar meaning are close to each other in the multidimensional vector space. The goal of this is to capture the semantic meaning of the words. For example, words that appear in a similar context most often have similar meanings and are thus in proximity to one another in the vector space.\n","\n","Glove (Global Vectors for Word Representation)'s main idea is to use a co-occurrence matrix to capture the relation between words. In a co-occurence matrix the rows and columns are the words in the vocabulary and the values are the number of times the words co-occur *in the same context* such as the same sentence or paragraph.\n","\n","GloVe then learns to factorize this co-occurrence matrix into two low-rank matrices, representing the words and their contexts, respectively. The resulting word vectors are the rows of the word matrix, while the column vectors of the context matrix represent the context of the corresponding words.\n","\n","The training objective of the GloVe model is to minimize the difference between the dot product of two word vectors and the log of their co-occurrence count, across all pairs of words in the corpus. This allows the model to learn word vectors that capture the semantic relationships between words and learn things such as analogies or synonyms.\n","\n","One of the most brilliant GloVe features is that by representing words as vectors, you can do mathematical operations on them such as adding, subtracting, etc. This allows us to do things such as finding the closest words to a given word, finding the odd one out in a list of words, etc.\n","\n","To demonstrate, I have loaded GloVe and used PCA to reduce the dimensionality of the vectors from 50 to 2 so that we can visualize them and see the clusters of words that have common traits ('colors', 'capitals', 'days of the week')"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":12044,"status":"ok","timestamp":1700632094026,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"3SL-T-KwxbKg","outputId":"04a16381-bef6-4b83-c133-c34a4c9d4c42"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","import os\n","\n","dir_root = '/content/drive/MyDrive/PredictNextWords'\n","glove_dir = 'glove/glove.6B'\n","glove_file = 'glove.6B.50d.txt'\n","# Load GloVe embeddings\n","word_embeddings = {}\n","with open(f'{dir_root}/{glove_dir}/{glove_file}', \"rb\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0].decode('utf-8')\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        word_embeddings[word] = coefs\n","\n","# Define words to compare\n","words = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n","         'paris', 'london',  'berlin', 'madrid', 'red', 'blue', 'green', 'yellow', 'black', 'white']\n","\n","# Get their corresponding embeddings\n","word_vectors = [word_embeddings[w] for w in words]\n","\n","# Reduce dimensionality to 2D using PCA\n","pca = PCA(n_components=2)\n","pca_vectors = pca.fit_transform(word_vectors)\n","\n","# Plot the 2D embeddings\n","x = pca_vectors[:, 0]\n","y = pca_vectors[:, 1]\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)\n","\n","# Add labels to the scatter plot\n","for i, word in enumerate(words):\n","    ax.annotate(word, (x[i], y[i]))\n","plt.title('Two dimensionsal (2D) representation of word embeddings')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7rXM3xWbxbKg"},"source":["Another interesting thing we can do with word embeddings is perform mathematical operations on the vectors whilst still preserving the meaning of the words. For example, assuming that these high dimensional vectors preserve meaning well, the distance between a *'boy'* and a *'girl'* should be similar to the distance between a *'king'* and a *'queen'*.\n","\n","Moreover, the vector obtained with *'queen'* - *'girl'* + *'boy'* should be similar to the vector of *'king'*. Lets check if these statements are true below."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":6771,"status":"ok","timestamp":1700632100790,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"_5bTDNCHxbKg","outputId":"7fa5c87b-0d94-4c8a-c24d-eaa39f616f4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine similarity between the king vector and the queen-girl+boy vector:  1.0000001\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","\n","# load glove embeddings again\n","word_embeddings = {}\n","with open(f'{dir_root}/{glove_dir}/{glove_file}', \"rb\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0].decode('utf-8')\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        word_embeddings[word] = coefs\n","\n","# Define the words to perform the operation on\n","# Define the words to perform the operation on\n","queen_vec = word_embeddings['queen']\n","girl_vec = word_embeddings['girl']\n","boy_vec = word_embeddings['boy']\n","king_vec = queen_vec - girl_vec + boy_vec\n","\n","# Calculate cosine similarity between the king vector and the \"queen - girl + boy\" vector\n","cosine_sim = np.dot(king_vec, (queen_vec - girl_vec + boy_vec)) / \\\n","    (np.linalg.norm(king_vec) * np.linalg.norm(queen_vec - girl_vec + boy_vec))\n","\n","print(\"Cosine similarity between the king vector and the queen-girl+boy vector: \", cosine_sim)\n","\n","# Get the corresponding embeddings\n","word_vectors = [queen_vec, girl_vec, boy_vec, king_vec]\n","\n","# Reduce dimensionality to 2D using PCA\n","pca = PCA(n_components=2)\n","pca_vectors = pca.fit_transform(word_vectors)\n","\n","# Plot the 2D embeddings\n","x = pca_vectors[:, 0]\n","y = pca_vectors[:, 1]\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)\n","\n","# Add lines connecting the \"queen\", \"girl\", \"boy\", and \"king\" points\n","ax.plot([x[0], x[3]], [y[0], y[3]], 'r--')\n","ax.plot([x[1], x[3]], [y[1], y[3]], 'g--')\n","ax.plot([x[2], x[3]], [y[2], y[3]], 'b--')\n","\n","# Add labels to the scatter plot\n","words = ['queen', 'girl', 'boy', 'king']\n","for i, word in enumerate(words):\n","    ax.annotate(word, (x[i], y[i]))\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"zCubnlmoxbKh"},"source":["As we can see, the value of *cosine similarity* $ = \\frac{(A . B)}{(||A|| ||B||)} = 1$ which indicates that the two vectors are identical. -1 would indicate oposite directions whilst 0 would indicate orthogonal vectors.\n","\n","\n","Finally, we can use the pre-trained GloVe word vectors are as a form of pretrained word embeddings in our model. Below I have written code that loads the GloVe vector and creates an embeddings matrix which we will use as the weights of our embedding layer in our model.\n","\n","The embeddings matrix is of shape (V, GD) where V is the size of the vocabulary and GD is the dimensionality of the GloVe vectors. Thus, the embedding layer will have an input dimension of V and an output dimension of GD. The weights of the embedding layer will be initialized with the embeddings matrix. This means that the embedding layer will not be trained and will only be used to map the input tokens to their corresponding GloVe vectors. In the embedding matrix, ith row corresponds to the ith vector in our word index dictionary."]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700632100791,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"TEALyZOpxbKh"},"outputs":[],"source":["def load_glove(word_index, root_dir = \"\", glove_dir='glove/glove.6B', embedding_dim=50):\n","    \"\"\"Loads the GloVe embedding matrix.\n","\n","    Parameters\n","    ----------\n","    word_index : dict\n","        A dictionary with the word index. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n","    glove_dir : str, optional\n","        The directory where the GloVe embeddings are stored, by default 'glove/glove.6B'.\n","        Can be downloaded from https://nlp.stanford.edu/projects/glove/.\n","    embedding_dim : int, optional\n","        The dimension of the GloVe embeddings, by default 50\n","\n","    Returns\n","    -------\n","    embedding_matrix : np.array\n","        A numpy array with the embedding matrix. Dimensions: (num_words, embedding_dim)\n","        Used to initialize the embedding layer in the model. ith row corresponds to the ith vector in the word_index.\n","    \"\"\"\n","\n","    import os\n","    import numpy as np\n","\n","    try:\n","        embeddings_index = {}\n","        f = open(f'{root_dir}/{glove_dir}/glove.6B.{embedding_dim}d.txt', encoding='utf-8')\n","        for line in f:  # Each line is a word and its embedding vector\n","            values = line.split()\n","            word = values[0]\n","            # The embedding vector\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            # Add the word and its embedding vector to the dictionary\n","            embeddings_index[word] = coefs\n","        f.close()\n","\n","        print(f'\\nLoaded glove. Found {len(embeddings_index)} word vectors.')\n","\n","        # prepare embedding matrix\n","        # The number of words in the word_index + 1 (for the 0th index)\n","        num_words = len(word_index) + 1\n","        embedding_matrix = np.zeros((num_words, embedding_dim))\n","        for word, i in word_index.items():\n","            embedding_vector = embeddings_index.get(word)\n","            if embedding_vector is not None:\n","                # words not found in embedding index will be all-zeros.\n","                # ith row corresponds to the ith vector in the word_index\n","                embedding_matrix[i] = embedding_vector\n","        print(\n","            f'Created embedding matrix. Dimensions: {embedding_matrix.shape}.')\n","        return embedding_matrix\n","    except Exception as e:\n","        print('Error occured:', e)\n"]},{"cell_type":"markdown","metadata":{"id":"R0x2iRegxbKh"},"source":["## Previous model: Stacked LSTMs\n","\n","### The text below is directly copied from the previous paper and is here for contextual and comparison purposes. You can skip directly to \"Improved architecture: Sequence to Sequence with Attention and LSTM\" for the new architecture.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wJzZMUmD486-"},"source":["### RNNs\n","\n","To understand LSTMs we must firstly have some understanding of what RNNs are. They are neural networks architecture designed to process sequential data such as text, time-series data, speech, etc. We call them recurrent because they operate as sort of a feedback loop allowing information from one tep in the sequence to be passed and used in the next step, which makes them great for sequential data and data of variable length.\n","\n","Below we have a visualization of a RNN courtesy of Towards AI.\n","\n","![RNN](https://drive.google.com/uc?export=view\u0026id=1ikuY6gDEWqPvmYiI2f8aHmKaHLoHQBwE)\n","\n","They are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations.\n","\n","Now, the vanishing gradient problem happens when the gradients become too small and close to 0 to be of any use for updating the weights of the network. As such, the network becomes unable to learn any long-term dependencies in the data since the information from the very early time steps is lost as the information gets propagated forward through the network. Consequently, RNNs may perform on tasks that require long-term memory, which we frequently need when analyzing time series or text.\n","\n","Mathematically, the vanishing gradients happen due to backpropagation. Specifically, we take the gradient of the loss function with respect to the weights in the network and multiply it by the gradient of the activation function with respect to the output of the previous layer. This is repeated for every layer in the network. Sometimes these derivatives are small (for example - when we have a sigmoid function as the activation function) and they become even smaller as we multiply them by the gradients of the previous layers. This means that the gradients get smaller and smaller and as the result the weights of the network are not updated properly and the network is unable to learn long-term dependencies.\n"]},{"cell_type":"markdown","metadata":{"id":"Wc9BvBqBxbKh"},"source":["I have decided to use an LSTM (Long Short-Term Memory) model architecture which is a type of recurrent neural network (RNN) architecture designed to model sequential data. It has been developed and mitigate the problem of vanishing gradients in vanilla RNNs.\n"]},{"cell_type":"markdown","metadata":{"id":"ycZcwGDt40h_"},"source":["### LSTM\n","To mitigate this problem, [Hochreiter, S., \u0026 Schmidhuber, Jurgen. (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf) introduced Long Short-Term memory.\n","\n","![LSTM](https://drive.google.com/uc?export=view\u0026id=1rVs_-qYcEY0cLMc1Iri5GsNUs_JUn1OX) Visualization by [Saul Dobilas](https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e).\n","\n","In the LSTM, every time step of the input sequence is processed by a cell consisting of several important components:\n","\n","1. *Input gate* - controls what information from the input sequence is passed to the cell state.\n","$ i_t = \\sigma(W_{xi}x_t + U_{hi} \\times h_{t-1} + b_i) $\n","2. *Forget gate* - controls what information from the cell state is passed to the next time step.\n","$ f_t = \\sigma(W_{xf}x_t + U_{hf} \\times h_{t-1} + b_f) $\n","3. *Output gate* - controls what information from the cell state gets passed to the output sequence.\n","$ o_t = \\sigma(W_{xo}x_t + U_{ho} \\times h_{t-1} + b_o) $\n","4. *Memory cell* - stores the information about the input sequence. Can be thought of as a conveyor belt that carries information from one time step to the next whilst selectively removing or adding information to the cell state.\n","$ \\tilde{C}_t = f_t \\times c_{t_1} + i_t \\times \\tanh(W_cx \\times x_t + U_ch \\times h_{t-1} + b_c)$\n","5. *Hidden state* - the output of the LSTM cell and is passed to the next time step.\n","$ h_t = o_t \\times \\tanh(\\tilde{C}_t)$\n","\n","\n","$x_t$ is the input to the cell at time step $t$, $h_{t-1}$ is the hidden state from the previous time step, $i_t$ is the input gate at $t$, $f_t$ is the forget gate at $t$, $o_t$ is the output gate at $t$, $c_{t-1}$ is the cell state from the previous time step, $\\tilde{C}_t$ is the cell state at $t$, $h_t$ is the hidden state at $t$.\n","\n","$W_{xi}$, $W_{xf}$, $W_{xo}$, $W_{xc}$ are the weights of the input gate, $U_{hi}$, $U_{hf}$, $U_{ho}$, $U_{hc}$ are the weights of the hidden state, $b_i$, $b_f$, $b_o$, $b_c$ are the biases of the gates.\n","\n","The input gate $i_t$ controls how much of the new input $x_t$ should be added to the cell state $c_{t-1}$. The forget gate $f_t$ controls how much of the previous cell state $c_{t-1}$ should be retained and passed on to the nexts time step, and how much should be discardedd. The output gate $o_t$ controls how much of the cell state $c_t$ should be output to the next layer or the final output of the network.\n","\n","The memory cell $c_t$ is updated based on the input $x_t$, the previous cell state $c_{t-1}$, and the values of the input and forget gates. The tanh function introduces nonlinearity and allows the model to capture more complex patterns in the data.\n","\n","The hidden state $h_t$ is the final output of the LSTM cell, and is calculated by applying the output gate to the current cell state $c_t$ and passing the result through the tanh function. The hidden state can be used as the input to the next LSTM cell in the sequence or as the final output of the network.\n","\n","Going back to the vanishing gradient problem, LSTMs mitigate it through the memory cell explained above which controlls the input, forget and the output gate and allows the model to retain information for a long time. Moreover, since the gates use sigmoid and tanh activation function, the values are always between 0 and 1 allowing control of information through the memory cell without them being too small or too large."]},{"cell_type":"markdown","metadata":{"id":"t9ph1_hz4w2C"},"source":["\n","\n","#### Training the LSTM\n","\n","To train them, we use regular backpropagation with the goal of minimizing the loss that measures the difference between the the predictions and true output for the given input. Important aspect of sequential dat is that both the input and the output have a temporal relationship which must not be broken.  Typically, we use a metric such as a mean squared error or, in this case, categorical cross entropy loss, since our goal is to predict the probability distribution over the vocabulary (categorical variable) given the previous words in the sentence.\n","\n","The loss function is calculated as follows:\n","$$H(p, q) = -\\sum_{i} p(i) \\log q(i)$$\n","\n","where $p$ is the true probability distribution over the categories and $q$ is the predicted probability distribution over the categories. In our case, $p$ is the one-hot encoded vector of the true word (1 for the target word and 0 for all others) and $q$ is the probability distribution over the vocabulary.\n","\n","The negative sign in the equation ensures that the loss is minimized by increasing the predicted probability of the true word and decreasing the predicted probabilities of all other words.\n","\n","By minimizing the categorical cross-entropy loss during training, the model learns to predict the correct probability distribution over the vocabulary given the previous words in the sentence allowing the model to generate more accurate predictions.\n","\n","This model is fully implemented under the function *train_lstm* below."]},{"cell_type":"markdown","metadata":{"id":"oL8_GKS-xbKi"},"source":["The function *train_lst* below is used to train the model. The model has the following layers. Stacking two LSTM layers next to one another greatly improved my performance. The final layers is a softmax layer since it gives us the probability distribution over the vocabulary.\n","\n","\n","The layer hyperparameters were chosen based on the tuning I did through the validation set. The following hyperparameters yielded the greatest validation accuracy.\n","\n","The following code includes several additional features. I have added both early stopping and model checkpointing. Early stopping is used to prevent overfitting by stopping the training process when the validation loss stops improving. Model checkpointing is used to save the model with the best validation accuracy.\n","\n","I have also added dropout layers to prevent overfitting. This is a regularization technique that randomly drops out units (along with their connections) from the neural network during training. This prevents units from co-adapting too much.\n","\n","Besides dropout layers I have also added an l2.regularizer. This is a regularization technique that adds a penalty to the loss function for large weights. This helps prevent overfitting by forcing the model to learn smaller weights.\n","\n","For the optimizer, I used Adam with the learning rate of 0.001 which proved the best in the validation set. Adam is preferred here to a static learning rate since it has an adaptive learning rate that changes for each parameter. This allows the model to converge faster and more efficiently. It also incorporates momentum by updating the moving average of the gradient and the moving average of the squared gradient once again leading to faster convergence and also helping in leaving local optima.\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1700632100791,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"I8y5cynNxbKi"},"outputs":[],"source":["def train_lstm(word_index, embedding_matrix, X_train, X_test, X_val, y_train, y_test, y_val, epochs=100, batch_size=256, lr=0.001, embedding_dim=100, seq_len=30, dropout_rate=0.2, weight_decay=1e-4):\n","    \"\"\"\n","    Trains the LSTM model. It also saves the model and the history as well as\n","\n","    Parameters\n","    ----------\n","    word_index : dict\n","        A dictionary with the word index. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n","    embedding_matrix : np.array\n","        A numpy array with the embedding matrix. Dimensions: (num_words, embedding_dim)\n","    epochs : int, optional\n","        The number of epochs to train the model, by default 100\n","    X_train : list\n","        A list of training features.\n","    X_test : list\n","        A list of testing features.\n","    X_val : list\n","        A list of validation features.\n","    y_train : list\n","        A list of training targets.\n","    y_test : list\n","        A list of testing targets.\n","    y_val : list\n","        A list of validation targets.\n","    batch_size : int, optional\n","        The batch size, by default 256\n","    lr : float, optional\n","        The learning rate, by default 0.001\n","    embedding_dim : int, optional\n","        The dimension of the GloVe embeddings, by default 100\n","    seq_len : int, optional\n","        The length of the sequences, by default 30\n","    dropout_rate : float, optional\n","        The dropout rate, by default 0.2\n","    weight_decay : float, optional\n","        The weight decay, by default 1e-3\n","\n","    Returns\n","    -------\n","    model : keras model\n","        The trained model.\n","    \"\"\"\n","\n","    from keras.models import Sequential\n","    from keras.layers import Embedding, LSTM, Dense, Dropout\n","    from keras.optimizers import Adam\n","    from keras.callbacks import ModelCheckpoint, EarlyStopping\n","    from keras import regularizers\n","    from pickle import dump\n","    from keras.utils.vis_utils import plot_model\n","\n","    num_words = len(word_index) + 1  # +1 because of the 0 index\n","\n","    print(\"\\nStarting LSTM model training...\")\n","\n","    model = Sequential()\n","    model.add(Embedding(num_words, embedding_dim, weights=[\n","        embedding_matrix], input_length=seq_len, trainable=False))\n","    model.add(LSTM(units=50, return_sequences=True,\n","              kernel_regularizer=regularizers.l2(weight_decay)))  # used to prevent overfitting by adding a penalty to the loss function\n","    model.add(Dropout(dropout_rate))  # used to prevent overfitting\n","    model.add(LSTM(50, kernel_regularizer=regularizers.l2(weight_decay)))\n","    model.add(Dropout(dropout_rate))  # used to prevent overfitting\n","    model.add(Dense(50, activation='relu',\n","              kernel_regularizer=regularizers.l2(weight_decay)))\n","    model.add(Dense(num_words, activation='softmax'))\n","    plot_model(model, to_file='model_plot.png',\n","               show_shapes=True, show_layer_names=True)\n","    model.summary()\n","\n","    # compile model\n","    my_opt = Adam(learning_rate=lr)\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=my_opt,\n","                  metrics=['accuracy'])\n","\n","    checkpoint_path = \"training_1\"\n","\n","    # Create a callback that saves the model's weights\n","    cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n","                                  save_weights_only=True,\n","                                  verbose=0,\n","                                  save_freq='epoch')\n","    # patience is the number of epochs to wait before stopping, if the model is not improving.\n","    earlystopping = EarlyStopping(\n","        monitor='val_accuracy', verbose=0, patience=3, restore_best_weights=True)\n","\n","    import numpy as np\n","    X_train = np.array(X_train)\n","    y_train = np.array(y_train)\n","    X_val = np.array(X_val)\n","    y_val = np.array(y_val)\n","    X_test = np.array(X_test)\n","    y_test = np.array(y_test)\n","\n","    # fit model\n","    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n","    history = model.fit(X_train, y_train, validation_data=(\n","        X_val, y_val), batch_size=batch_size, epochs=epochs, shuffle=True, callbacks=[earlystopping, cp_callback])\n","\n","    # save the model to file\n","    model_name = f\"saved_models/lstm_{embedding_dim}d_{seq_len}seq_{epochs}epochs_{lr}lr_{batch_size}batch.h5\"\n","    model.save(model_name)\n","    history_name = f\"saved_models/lstm_{embedding_dim}d_{seq_len}seq_{epochs}epochs_{lr}lr_{batch_size}batch_history.pkl\"\n","    dump(history.history, open(history_name, 'wb'))\n","\n","    # evaluate model\n","    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n","    print()\n","    print('Test Loss: %f' % (loss))\n","    print('Test Accuracy: %f' % (accuracy))\n","\n","    return model, history\n"]},{"cell_type":"markdown","metadata":{"id":"_09gl_804FKg"},"source":["## Improved architecture: Sequence to Sequence with Attention and LSTM"]},{"cell_type":"markdown","metadata":{"id":"4YJguLRDCdzU"},"source":["### LSTM limitations\n","\n","To understand the proposed architecture, we must firstly understand the limitations of the architecture that we used in the previous project - LSTMs.\n","\n","Up until the publishing of\n","[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) by Bahdanau et al. RNNs, LSTMs and GRUs were considered the state of the art for most, if not all, sequence tasks.\n","\n","Note: GRUs that are more simplified versions of LSTMs that have only two gates: the reset gate and the update gate, which tend to achieve similar or better results to LSTMs with less parameters and faster training times.\n","\n","\n","However, Bahdanau et al. showed an architecture that is a stepping stone into some of the state-of-the-art techniques, such as transformers (famous paper \"Attention is all you need\" directly came out of the abovementioned paper), GPT models, etc.\n","\n","To understand the need for this model, we must firstly understand the limiations with the model we developed previously.\n","\n","Firstly, LSTMs still have difficulty in modeling long-term dependencies despite them being developed to address the vanishing gradient problem in traditional RNNs. However, testings we have done have shown that even LSTMs (even powerful ones that are stacked like we did) were ineffective in capturing long-term dependencies effectively, especially when the gap between the current word and the predicted word is large. This is especially true in the case of many-to-many relationships (we are doing many-to-one since we are just predicting the next word).\n","\n","Perhaps not really applicable to our case, but LSTMs require fixed length input sequences meaning that sentences that are much shorter or much longer than the ones the LSTMs have been trained one will give poor results since they will have to be cut off or padded with 0s.\n","\n","Most importantly, LSTMs lack context. Since their underlining architecture they still process each word isolation without really having an understanding of its context (we humans are amazing at remembering that \"He\" refers to \"Mark\" in the sentences \"Mark bought a car. He loves it\"). LSTMs are not really good at paying *attention* at these things, especially in very large texts, leading to poor results, especially in cases where there might be sublety and many interpretations.\n","\n","\n","\n","### Sequence to Sequence or the Encoder Decoder structure\n","\n","Much like a normal Encoder-Decoder strucutre, the vanilla Seq2Seq model has two parts:\n","\n","1. **Encoder**: takes in the sequence of words $(x1, x2, ..., xn)$ and encodes it in a fixed length vector $(c)$ which captures the meaning as best as possible. This is usually done through an LSTM or GRU cell much like we did above. The encoder hidden state $(h_i)$ at each time step $i$ is calculated as follows:\n","\n","$$h_i = LSTM(x_i, h_i-1)$$\n","\n","The final encoder output $(c)$ is the last hidden state of the encoder:\n","$c = h_n$\n","\n","2. **Decoder**: generates the output sequence $(y1, y2,..,ym)$ (which in our case it would be just a single word) based on the context vector $(c)$ that the encoder encoded.\n","\n","The decoder's hidden state $(s_j)$ at time $j$ given $$s_j = LSTM(y_j-1, s_j-1)$$\n","where $(s_j-1)$ is the hidden state $(y_j-1)$ is the previously predicted word\n","\n","The context vector $(c)$ is used to initialize the first hidden state $(s_0)$ of the decoder:\n","\n","$s_0 = c$\n","\n","The predicted output $(y_j)$ at each time step is obtained by passing the decoder hidden state $(s_j)$ through a softmax activation function:\n","\n","$$y_j = softmax(Ws_j + b)$$\n","\n","where $W$ and $b$ are learnable weights and biases.\n","\n","In the case of natural language translation from English to German, this is how the architecture looks.\n","\n","![](https://drive.google.com/uc?export=view\u0026id=1F_7qZs34F7w8S6eXN8py7oqNZnavfh7q)\n","\n","Mathematically, the encoder-decoder structure tries to approximate the mapping from one sequence to another (in our case, just to the next word). It learns to do so through supervised method by minimizing the difference between the predicting made with the decoder and the targets.\n","\n","The training is done by minimizing the cross-entropy loss through backpropagation and gradient descent:\n","\n","$$L = -\\sum_{j=1}^{m} [y_j^* \\log(y_j) + (1-y_j^*)\\log(1-y_j)]$$\n","\n","where $y_j^*$ is the one-hot encoded target output at time step $j$, and $y_j$ is the predicted output at time step $j$.\n","\n","\n","#### Limitations\n","\n","Unfortunately, even this architecture has its limitations similar to the vanilla LSTM. It also has problems dealing with long-term dependencies and it pays attention to every word in the input with equal importance when generating the output.  Interestinly, since the Seq2Seq model is trained to generate the next word given the ground-truth previous words, the model generates the previous predicted words instead of the ground-truth words, which oftentims leads to a compounding error known as exposure bias.\n","\n","To combat this, we introduce the concept of attention.\n","\n","\n","![](https://drive.google.com/uc?export=view\u0026id=1pHC636KJLLO7cuDc-jSSOaV2YIip6NBJ)\n","\n","Attention is essentially comprised of the following three steps:\n","\n","1. We calculate the scalar attention score between each decoder hidden state $(s_i)$ and each encoder hidden state $(h_j)$, which measures the relevance of the encoder token $j$ for predicting the decoder token $i$:\n","$$a_{i,j} = \\frac{\\text{exp}(s_i \\cdot h_j)}{\\sum_k \\text{exp}(s_i \\cdot h_k)}$$\n","where $s_i$ and $h_j$ are the decoder and encoder hidden states at time step i and j respectively, and $a_{i,j}$ is the attention score for decoder token i and encoder token j.\n","\n","2. Normalize the attention scores using a softmax function to obtain a probability distribution over the encoder tokens, which represent the importance of each token for predicting the next word:\n","$$w_{i,j} = \\frac{\\text{exp}(a_{i,j})}{\\sum_k \\text{exp}(a_{i,k})}$$\n","where $w_{i,j}$ is the attention weight for decoder token i and encoder token j\n","\n","3. Finally, calculate the weighted sum of encoder hidden states with attention weights and feed it into the next decoder cell:\n","$$c_i = \\sum_j w_{i,j} h_j$$\n","where $c_i$ is the context vector for decoder token i.\n","\n","\n","This structure provides much better results since it allows the decoder to selectively focus on the most important parts of the input sequence allowing it to better capture long term dependencies, better handle variable length input sequences. It is also more interpretable since we can visualize which part of the input sequence are most inportant (we just look at the weight associated with each word)\n"]},{"cell_type":"markdown","metadata":{"id":"NDixZ9x3uqgR"},"source":["### Loss function\n","\n","Since we essentially have a multi-class classification problems where the labels are the indexes in the vocabulary rather than one-hot encoded vectors like we had last time, we can use **Sparse categorical crossentropy**:\n","\n","$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{i,j} \\log(p_{i,j})$$\n","\n","where $N$ is the batch size, $C$ is the number of classes (words in this case - *vocabulary size*), $y_{i,j}$ is 1 if the true label of sample $i$ is class $j$ and 0 otherwise, and $p_{i,j}$ is the predicted probability of sample $i$ being in class $j$."]},{"cell_type":"markdown","metadata":{"id":"vF5uB3_RDxEv"},"source":["### Evaluation metrics\n","\n","As opposed to just the accuracy which only looks at the number of correctly made predictions out of total predictions, we will be using more sophisticated metrics. Accuracy is not sufficient since we can have a prediction that is good and reasonable but is not the one found in the training set. For example: *\"Today is a\"* can be followed by any of the following [\"great\", \"good\", \"rainy\", etc.]. Rather than just labelling all predictions but the target as incorrect, we can be more intelligent and look at if they actually make sense.\n","\n","To accomplish that, we shall use the following metrics:\n","\n","1. **Accuracy**: $$Accuracy = \\frac{number\\ of\\ correctly\\ predicted\\ words}{total\\ number\\ of\\ predicted\\ words}$$\n","\n","2. **Top-k accuracy**: \u003cbr\u003e Count the prediction as correct if the target word is within the top k predictions made (top k most probable words) by the model.\n","$$top\\ k\\ accuracy = \\frac{number\\ of\\ correctly\\ predicted\\ words\\ in\\ top\\ k\\ predictions}{total\\ number\\ of\\ predicted\\ words}$$\n","3. **Perplexity**: Perplexity is one of the most common metrics used in text generation to evaluate the quality of a language model by measuring how well it predicts a sample of previously unseen text.\n","$$\\text{Perplexity} = e^{H(y_{true}, y_{pred})} = e^{-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(y_{true}^{(i)} | x^{(i)}, \\theta)}$$\n","where $y_{true}$ is the true label, $y_{pred}$ is the predicted label, $N$ is the number of samples in the dataset, $x^{(i)}$ is the $i$-th input sample, and $\\theta$ are the model parameters. \u003cbr\u003e\n","It is just the exponentiated *cross-entropy loss* between the true and predicted labels $H(y_{true}, y_{pred})$:\n","$$H(y_{true}, y_{pred}) = -\\sum_{c=1}^{C} y_{true,c} \\log y_{pred,c}$$,where $C$ is the number of classes, $y_{true,c}$ is the true probability of the $c$-th class, and $y_{pred,c}$ is the predicted probability of the $c$-th class.\n","\n","4. **Cosine similarity**: This is something that is not usually used but I believe we can look at where the predicted word is located in the multidimensional vector space by looking at its embedding and then comparing how similar it is to the vector of the true label in that vector space using cosine similarity:\n","$$\\text{cosine_similarity}(A, B) = \\frac{A \\cdot B}{\\left\\Vert A \\right\\Vert \\left\\Vert B \\right\\Vert}$$\n","where $A$ and $B$ are the two vectors being compared, and $\\left\\Vert A \\right\\Vert$ and $\\left\\Vert B \\right\\Vert$ represent the Euclidean lengths (magnitude) of vectors $A$ and $B$, respectively. The dot product of vectors $A$ and $B$ is represented by $A \\cdot B$."]},{"cell_type":"markdown","metadata":{"id":"7_o4MIHJloYt"},"source":["### Model architecture"]},{"cell_type":"markdown","metadata":{"id":"5YdWfiRqYzj-"},"source":["Drawing from the explanation above, our model architecture will be slightly modified for our task and will consist of (visualization available below):\n","1. **input layer**: takes in the 100 word sequence $(w_1, w_2, \\dots, w_{100})$ as input. Output is tensor of shape: $[(None, 100)]$\n","2. **embedding layer**: takes in the inputs and converts them to their 50-dimensional pre-trained GloVe embeddings. These become the initial weights. Interesting, setting these to be trainable (which is not usually done) gives around a 1.5% increase in accuracy. I presume this is becasue of the small dataset which has its own quirky distribution.  Output is tensor of shape: $[(None, 100, 50)]$\n","3. **the encoder**: comprises of an LSTM cell that also has an L2 regulizer, and a weight dropout layer to prevent overfitting. It returns the output sequences for each timestep and not only the final one to allow the decoder to have access to the entire input sequence, which is necessary for generating the correct output sequence. Output is tensor of shape: $[(None, 100, 50)]$\n","4. **the decoder**: comprises of an LSTM cell that also has an L2 regulizer, and a weight dropout layer to prevent overfitting. However, it returns only the output of the last timestep, rather than the output sequence of all timesteps. This is then added with the updated context vector from the attention layer (in the concatenation layer later). Output is tensor of shape: $[(None, 50)]$.\n","5. **the attention layer**: Comprising of a custom attention layer which we will define below (this is because we can not use the ones available in Keras since they are made only for long sequence generation and not for just a single word). The attention layer will take the output of the *encoder* and *decoder* and will produce the context vector which will be a tensor of shape: $[(None, 50)]$\n","6. **the concatenation layer**: sums the attention context vector and the decoder hidden state (its output) as explained above. Output is tensor of shape: $[(None, 100)]$\n","7. **dropout layer**: attaches to the concatenation layer and randomly sets a fraction of the input units to zero, which effectively drops them out of the network for that training epoch thereby reducing overfitting. Output is tensor of shape: $[(None, 100)]$\n","8. **the dense layer**: this is just a regular dense layer with softmax activation function applied to it so we get a probability distribution for each word. The index with the highest value represent the word with the highest probability of being the next word. As such, its output is tensor of shape: $[(None, Vocabulary \\text{ } size)]$\n","\n","Below is the final architecture of the model including the inputs and outputs of each layer that also showcases the changes in dimensionality.\n","![](https://drive.google.com/uc?export=view\u0026id=1fzGpkUeqBEwzvQNt2_S9jYgMsggeJsuN)\n","\n","\n","The code for the arcitecture above is implemented in the cell below."]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":10324,"status":"ok","timestamp":1700632111108,"user":{"displayName":"Quân Đặng Minh","userId":"12290221164976792771"},"user_tz":-420},"id":"N9AyqwqG0i6K"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, TimeDistributed, Attention\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras import regularizers\n","from keras.utils import plot_model\n","from pickle import dump\n","\n","def seq2seq_with_attention(word_index, embedding_matrix, X_train, X_test, X_val, y_train, y_test, y_val, epochs=100, batch_size=256, lr=0.001, embedding_dim=100, seq_len=30, dropout_rate=0.2, weight_decay=1e-4, root_dir=\"\"):\n","    \"\"\"\n","    Trains a Sequence to Seqence with Attention and LSTMs model for next word prediction.\n","    It predicts only a single word (the next word y_1) and not a sequence of words (y_1, y_2, y_3, y_4).\n","\n","\n","    Parameters\n","    ----------\n","    word_index : dict\n","        A dictionary with the word index. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n","    embedding_matrix : np.array\n","        A numpy array with the embedding matrix. Dimensions: (num_words, embedding_dim)\n","    epochs : int, optional\n","        The number of epochs to train the model, by default 100\n","    X_train : list\n","        A list of training features. Shape: (num_samples, seq_len)\n","    X_test : list\n","        A list of testing features. Shape: (num_samples, seq_len)\n","    X_val : list\n","        A list of validation features. Shape: (num_samples, seq_len)\n","    y_train : list\n","        A list of training targets. Shape: (num_samples, ). It is a list of indices that correspond to the next word for each sequence.\n","    y_test : list\n","        A list of testing targets. Shape: (num_samples, ).\n","    y_val : list\n","        A list of validation targets. Shape: (num_samples, )\n","    batch_size : int, optional\n","        The batch size, by default 256\n","    lr : float, optional\n","        The learning rate, by default 0.001\n","    embedding_dim : int, optional\n","        The dimension of the GloVe embeddings, by default 100\n","    seq_len : int, optional\n","        The length of the sequences, by default 30\n","    dropout_rate : float, optional\n","        The dropout rate, by default 0.2\n","    weight_decay : float, optional\n","        The weight decay, by default 1e-3\n","    root_dir : str, optional\n","        The root directory, by default \"\"\n","        In case of Google Colab, the root directory is \"/content/drive/My Drive/\"\n","\n","    Returns\n","    -------\n","    model : keras model\n","        The trained model.\n","    history : dictionary\n","        A dictionary containing the training history of the model.\n","    \"\"\"\n","\n","    def custom_attention():\n","        \"\"\"\n","        Since most Attention function available in Keras only works for predicting long sequences,\n","        we need to create a custom attention layer that works for predicting a single word.\n","\n","        We use the Bahdanau Attention mechanism as described in the paper\n","        \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n","        by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\n","\n","        \"\"\"\n","        # inherit from the super class tf.keras.layers.Layer (gives us access to the add_weight() method\n","        class CustomAttention(tf.keras.layers.Layer):\n","            def __init__(self):\n","                \"\"\"\n","                Initialize the layer.\n","                \"\"\"\n","                super(CustomAttention, self).__init__()  # call the super class constructor and initialize the layer\n","\n","            def build(self, input_shape):\n","                \"\"\"\n","                Initialize the weights of the layer. self.add_weight() is inherited from the super class tf.keras.layers.Layer.\n","\n","                W1 and W2 are the weights of the encoder and decoder respectively. Shape = (embedding_dim, embedding_dim)\n","                V is the weight of the attention layer.\n","\n","                They are initialized with the Glorot uniform initializer because it is recommended for RNNs as per the paper\n","                \"Understanding the difficulty of training deep feedforward neural networks\" by Xavier Glorot and Yoshua Bengio.\n","\n","                Parameters\n","                ----------\n","                input_shape : tuple\n","                    A tuple containing the input shape.\n","\n","                Returns\n","                -------\n","                None\n","                \"\"\"\n","                self.W1 = self.add_weight(shape=(\n","                    input_shape[0][-1], input_shape[0][-1]), initializer='glorot_uniform', name='W1')  # add the weight to the layer\n","                self.W2 = self.add_weight(shape=(\n","                    input_shape[1][-1], input_shape[1][-1]), initializer='glorot_uniform', name='W2')  # add the weight to the layer\n","                self.V = self.add_weight(\n","                    shape=(input_shape[0][-1], 1), initializer='glorot_uniform', name='V')  # add the weight to the layer\n","\n","            def call(self, inputs):\n","                \"\"\"\n","                Compute the context vector and return it.\n","                The context vector is calculated by\n","\n","                Parameters\n","                ----------\n","                inputs : list (tf.tensor, tf.tensor) represetning (encoder_outputs, decoder_hidden)\n","                    A list containing the encoder outputs and decoder hidden state.\n","                    The encoder outputs are the outputs of the encoder LSTM.\n","                    The decoder hidden state is the output of the decoder LSTM.\n","\n","                Returns\n","                -------\n","                context_vector : tf.tensor\n","                    The context vector.\n","                \"\"\"\n","                encoder_outputs, decoder_hidden = inputs  # unpack the inputs\n","                # expand the dimension of the decoder hidden state to match the encoder outputs\n","                decoder_hidden = tf.expand_dims(decoder_hidden, 1)\n","                # compute the scalar attention score: tanh(W1*encoder_outputs + W2*decoder_hidden)\n","                score = tf.nn.tanh(\n","                    tf.matmul(encoder_outputs, self.W1) + tf.matmul(decoder_hidden, self.W2))\n","                # normalize the attention scores using a softmax function to obtain a probability distribution over the encoder tokens\n","                attention_weights = tf.nn.softmax(\n","                    tf.matmul(score, self.V), axis=1)\n","                # compute the context vector by multiplying the attention weights with the encoder outputs and summing them up\n","                context_vector = attention_weights * encoder_outputs\n","                context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","                return context_vector\n","        return CustomAttention()  # return the layer\n","\n","    def top_k_accuracy(y_true, y_pred):\n","        \"\"\"\n","        Compute the top k accuracy of the model using sparse categorical crossentropy (for one-hot encoded labels)\n","        A prediction is considered accurate if the true label is in the top k predictions.\n","\n","        Parameters\n","        ----------\n","        y_true : tf.tensor. shape: (batch_size, 1)\n","            batch_size is the number of samples in a batch. Each element is an integer representing the index of the true class.\n","        y_pred : tf. tensor. shape: (batch_size, vocab_size).\n","            each row is a probability distribution over the vocab_size, summing to 1.\n","            The index of the maximum value in each row is the predicted class.\n","\n","        Returns\n","        -------\n","        Sparse top K categorical accuracy value. Float between 0 and 1.\n","        \"\"\"\n","\n","        from keras.metrics import sparse_top_k_categorical_accuracy\n","\n","        top_k = 5  # top 5 accuracy\n","        return sparse_top_k_categorical_accuracy(y_true, y_pred, k=top_k)\n","\n","    def perplexity(y_true, y_pred):\n","        \"\"\"\n","        Compute the perplexity of the model using sparse categorical crossentropy (for one-hot encoded labels)\n","        Perplexity is just exponentiated categorical crossentropy or the average negative log-likelihood of a sequence of words.\n","\n","        In the case of predicting a single word, there is no context or sequence of words to evaluate the language model against.\n","        The model is simply making a prediction based on a single input token.\n","        Therefore, calculating perplexity for a single word prediction would not be meaningful because it does not capture the\n","        ability of the model to predict future words in the sequence. Thus, it is expected that the perplexity of the model would\n","        be increasing rapidly as the model is trained.\n","\n","        Parameters\n","        ----------\n","        y_true : tf.tensor. shape: (batch_size, 1)\n","            batch_size is the number of samples in a batch. Each element is an integer representing the index of the true class.\n","        y_pred : tf. tensor. shape: (batch_size, vocab_size).\n","            each row is a probability distribution over the vocab_size, summing to 1.\n","            The index of the maximum value in each row is the predicted class.\n","\n","        Returns\n","        -------\n","        perplexity : float\n","            The perplexity of the model.\n","        \"\"\"\n","\n","        import keras.backend as K\n","\n","        cross_entropy = K.sparse_categorical_crossentropy(\n","            y_true, y_pred)  # compute the sparse categorical crossentropy\n","        # exponentiate the crossentropy to obtain the perplexity\n","        perplexity = K.exp(cross_entropy)\n","        return perplexity\n","\n","\n","    def wrapped_cosine_similarity(embedding_matrix):\n","        \"\"\"\n","        Compute the cosine similarity between the true and predicted labels.\n","\n","        The reason why this is wrapped into another function is because of\n","        how the custom metrics are implemented in Keras. Since I have to pass\n","        the embedding matrix to this metric, I have to wrap it into a metric\n","        as per https://stackoverflow.com/a/71422348\n","\n","        Parameters\n","        ----------\n","        y_true : tf.tensor. shape: (batch_size, 1)\n","            batch_size is the number of samples in a batch. Each element is an integer representing the index of the true class.\n","        y_pred : tf. tensor. shape: (batch_size, vocab_size).\n","            each row is a probability distribution over the vocab_size, summing to 1.\n","            The index of the maximum value in each row is the predicted class.\n","        embedding_matrix: tf.tensor. shape: (vocab_size, embedding_dim)\n","            Embedding matrix of vocabulary size and embedding dimension\n","\n","        Returns\n","        -------\n","        cosine_similarity : float (between -1 and 1)\n","            The cosine similarity between the true and predicted labels.\n","        \"\"\"\n","\n","        def cosine_similarity(y_true, y_pred):\n","          # get the index of the maximum value in each row of y_pred\n","          y_pred = tf.argmax(y_pred, axis=1)\n","\n","          # convert y_true to int32 data type\n","          y_true = tf.cast(y_true, dtype=tf.int32)\n","\n","          # expand the dimensions of y_true and y_pred\n","          y_true = tf.expand_dims(y_true, -1)\n","          y_pred = tf.expand_dims(y_pred, -1)\n","\n","          # get the embedding of the true and predicted labels\n","          y_true = tf.nn.embedding_lookup(embedding_matrix, y_true)\n","          y_pred = tf.nn.embedding_lookup(embedding_matrix, y_pred)\n","\n","          # compute the cosine similarity between the true and predicted labels\n","          numerator = tf.reduce_sum(tf.multiply(y_true, y_pred), axis=1)\n","          denominator = (tf.sqrt(tf.reduce_sum(tf.square(y_true), axis=1)) *\n","                        tf.sqrt(tf.reduce_sum(tf.square(y_pred), axis=1)))\n","          cosine_similarity = tf.where(tf.math.is_finite(numerator / denominator),\n","                                      numerator / denominator,\n","                                      tf.zeros_like(numerator))\n","\n","          # normalize the cosine similarity to be between -1 and 1\n","          cosine_similarity = tf.clip_by_value(cosine_similarity, -1.0, 1.0)\n","          return tf.reduce_mean(cosine_similarity)\n","\n","        return cosine_similarity\n","\n","    def build_model():\n","        \"\"\"\n","        Builds the model consisting of:\n","        - the embedding layer (converts the inputs into pretrained GloVe embeddings); trainable weights give better results\n","        - the encoder (comprising of an LSTM cell),\n","        - the decoder (comprising of an LSTM cell),\n","        - the attention layers (comprising of a custom attention layer defined above connecting the encoder and decoder),\n","        - the concatenation layer (the attention context vector + decoder hidden state),\n","        - dropout layer (comprising of dropout layer to prevent overfitting), attaches to the concatenation layer\n","        - the dense layer (comprising of a dense layer with softmax activation function), attaches to the dropout layer\\\n","\n","        Loss function: sparse categorical crossentropy (for one-hot encoded labels)\n","        Optimizer: Adam\n","        Metrics: accuracy, top k accuracy, cosine similarity, perplexity\n","        Both LSTM cells have a dropout rate and a weight decay (L2 regularization)\n","\n","        Prints the model summary and saves the model architecture as a png file.\n","\n","        Returns\n","        -------\n","        model : tf.keras.Model\n","            The model.\n","        \"\"\"\n","\n","        # input Layer\n","        input_layer = Input(shape=(seq_len,))\n","\n","        # embedding Layer (pre-trained GloVe embeddings), not trainable so that the weights are not updated during training\n","        embedding_layer = Embedding(input_dim=len(word_index) + 1,\n","                                    output_dim=embedding_dim,\n","                                    weights=[embedding_matrix],\n","                                    input_length=seq_len,\n","                                    trainable=True)(input_layer)\n","\n","        # encoder: LSTM cell with dropout and weight decay (L2 regularization)\n","        encoder_lstm = LSTM(units=embedding_dim,\n","                            return_sequences=True,\n","                            dropout=dropout_rate,\n","                            kernel_regularizer=regularizers.l2(weight_decay))(embedding_layer)\n","\n","        # decoder: LSTM cell with dropout and weight decay (L2 regularization)\n","        decoder_lstm = LSTM(units=embedding_dim,\n","                            return_sequences=False,\n","                            dropout=dropout_rate,\n","                            kernel_regularizer=regularizers.l2(weight_decay))(encoder_lstm)\n","\n","        # attention Layer\n","        attention_layer = custom_attention()([encoder_lstm, decoder_lstm])\n","        concat_layer = Concatenate(axis=-1)([decoder_lstm, attention_layer])\n","\n","        # dense layer\n","        dropout_layer = Dropout(rate=dropout_rate)(concat_layer)\n","        output_layer = Dense(len(word_index) + 1,\n","                             activation='softmax')(dropout_layer)\n","\n","        # create the Model\n","        model = Model(inputs=input_layer, outputs=output_layer)\n","\n","        # compile the Model\n","        model.compile(loss='sparse_categorical_crossentropy',\n","                      optimizer=Adam(learning_rate=lr),\n","                      metrics=['accuracy', top_k_accuracy, wrapped_cosine_similarity(embedding_matrix), perplexity])\n","\n","        # model Summary\n","        print(model.summary())\n","\n","        # plot the Model\n","        plot_model(model, to_file='model.png',\n","                   show_shapes=True, show_layer_names=True)\n","\n","        return model\n","\n","    def train_model(model):\n","        # making a model checkpoint every time the validation loss decreases\n","        checkpoint = ModelCheckpoint(\n","            filepath=f'{root_dir}/model_checkpoints/model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","        # stopping the training if the validation loss does not decrease for 'patience=10' epochs\n","        early_stopping = EarlyStopping(\n","            monitor='val_loss', patience=10, verbose=1, mode='min')\n","\n","        # train the model and saving the history\n","        history = model.fit(X_train, np.expand_dims(y_train, -1),\n","                            epochs=epochs,\n","                            batch_size=batch_size,\n","                            validation_data=(X_val, np.expand_dims(y_val, -1)),\n","                            callbacks=[checkpoint, early_stopping],\n","                            verbose=1)  # expand the dimensions of the validation and training labels to match the model output\n","\n","        # Load the best weights\n","        model.load_weights(f'{root_dir}/model_checkpoints/model.h5')\n","\n","        return model, history\n","\n","    def plot_results(history):\n","        \"\"\"\n","        Plots the training and validation loss and accuracy.\n","\n","        Parameters\n","        ----------\n","        history : keras history\n","            The history of the training.\n","        \"\"\"\n","        import matplotlib.pyplot as plt\n","\n","        # plot training and validation loss\n","        plt.plot(history.history['loss'], label='train')\n","        plt.plot(history.history['val_loss'], label='validation')\n","        plt.title('Training and validation loss')\n","        plt.ylabel('Loss')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.show()\n","\n","        # plot training and validation accuracy\n","        plt.plot(history.history['accuracy'], label='train')\n","        plt.plot(history.history['val_accuracy'], label='validation')\n","        plt.title('Training and validation accuracy')\n","        plt.ylabel('Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.show()\n","\n","        # plot training and validation top-5 accuracy\n","        plt.plot(history.history['top_k_accuracy'], label='train')\n","        plt.plot(history.history['val_top_k_accuracy'], label='validation')\n","        plt.title('Training and validation top-5 accuracy')\n","        plt.ylabel('Top-5 Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.show()\n","\n","        # plot training and validation cosine similarity\n","        plt.plot(history.history['cosine_similarity'], label='train')\n","        plt.plot(history.history['val_cosine_similarity'], label='validation')\n","        plt.title('Training and validation cosine similarity')\n","        plt.ylabel('Cosine Similarity')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.show()\n","\n","        # plot training and validation perplexity\n","        plt.plot(history.history['perplexity'], label='train')\n","        plt.plot(history.history['val_perplexity'], label='validation')\n","        plt.title('Training and validation perplexity')\n","        plt.ylabel('Perplexity')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.show()\n","\n","    # convert lists to numpy arrays\n","    X_train = np.array(X_train)\n","    y_train = np.array(y_train)\n","    X_val = np.array(X_val)\n","    y_val = np.array(y_val)\n","    X_test = np.array(X_test)\n","    y_test = np.array(y_test)\n","\n","    # build the model, train the model and plot the results\n","    model = build_model()\n","    trained_model, history = train_model(model)\n","    plot_results(history)\n","\n","    # evaluate the Model\n","    loss, accuracy, top_k_accuracy, cosine_similarity, perplexity = model.evaluate(X_test, np.expand_dims(\n","        y_test, -1), batch_size=batch_size, verbose=1)\n","\n","    # print the final test results\n","    print(\"Test loss:\", loss)\n","    print(\"Test accuracy:\", accuracy)\n","    print(\"Test top-5 accuracy:\", top_k_accuracy)\n","    print(\"Test perplexity:\", perplexity)\n","    print(\"Test cosine similarity:\", cosine_similarity)\n","\n","    # give the model a name\n","    model_name = f\"seq2seq_{embedding_dim}dGLOVE_{seq_len}seq_{epochs}epochs_{lr}lr_{batch_size}batch\"\n","\n","    # save the model\n","    trained_model.save(f'{root_dir}/saved_model/{model_name}.h5')\n","    # save the history\n","    model_name = f\"seq2seq_{embedding_dim}dGLOVE_{seq_len}seq_{epochs}epochs_{lr}lr_{batch_size}batch\"\n","    with open(f'{root_dir}/saved_histories/{model_name}.pkl', 'wb') as f:\n","        dump(history.history, f)"]},{"cell_type":"markdown","metadata":{"id":"C8D9imE3xbKl"},"source":["### Tying it all together"]},{"cell_type":"markdown","metadata":{"id":"Cq7dwsPUxbKl"},"source":["We tie all of the above mentioned functions together in the following code which follows the pipeline in the introduction. This allows for reproducibility and easy testing of different models and hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Yloe-55-zJmB"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Read master.txt. It contains 4369093 characters and 753885 words.\n","\n","Preprocessing text...\n","\tConverted to lowercase and stripped multiple whitespaces.\n","\tRemoved punctuation and numbers.\n","\tExpanding contractions using contractions model...\n","\tSplit the text into tokens.\n","\tLemmatized words.\n","\tPreprocessed text saved to /content/drive/MyDrive/PredictNextWords/database/processed/master.txt_processed.txt\n","Preprocessing finished. There are now 743750 tokens.\n","\n","Created word-to-index dictionary. Total number of unique tokens: 17638.\n","Created feature (100 words) and target (1 word) pairs. Total number of datapoints: 743650.\n","Split dataset into training (80.0%), validation (10.0%), testing(10.0%). Sizes: X_train: 594920, X_test: 74365, X_val: 74365\n","\n","Loaded glove. Found 400000 word vectors.\n","Created embedding matrix. Dimensions: (17639, 50).\n","\n","SEQUENCE_LENGTH: 100, EPOCHS: 50, BATCH_SIZE: 512, EMBEDDING_DIM: 50, LR: 0.002\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 100, 50)              881950    ['input_1[0][0]']             \n","                                                                                                  \n"," lstm (LSTM)                 (None, 100, 50)              20200     ['embedding[0][0]']           \n","                                                                                                  \n"," lstm_1 (LSTM)               (None, 50)                   20200     ['lstm[0][0]']                \n","                                                                                                  \n"," custom_attention (CustomAt  (None, 50)                   5050      ['lstm[0][0]',                \n"," tention)                                                            'lstm_1[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 100)                  0         ['lstm_1[0][0]',              \n","                                                                     'custom_attention[0][0]']    \n","                                                                                                  \n"," dropout (Dropout)           (None, 100)                  0         ['concatenate[0][0]']         \n","                                                                                                  \n"," dense (Dense)               (None, 17639)                1781539   ['dropout[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 2708939 (10.33 MB)\n","Trainable params: 2708939 (10.33 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/50\n","1162/1162 [==============================] - ETA: 0s - loss: 6.4210 - accuracy: 0.0659 - top_k_accuracy: 0.1752 - cosine_similarity: 0.1670 - perplexity: 68066.5391\n","Epoch 1: val_loss improved from inf to 5.96189, saving model to /content/drive/MyDrive/PredictNextWords/model_checkpoints/model.h5\n","1162/1162 [==============================] - 1900s 2s/step - loss: 6.4210 - accuracy: 0.0659 - top_k_accuracy: 0.1752 - cosine_similarity: 0.1670 - perplexity: 68066.5391 - val_loss: 5.9619 - val_accuracy: 0.1116 - val_top_k_accuracy: 0.2298 - val_cosine_similarity: 0.2393 - val_perplexity: 79848.5547\n","Epoch 2/50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["1162/1162 [==============================] - ETA: 0s - loss: 5.7941 - accuracy: 0.1236 - top_k_accuracy: 0.2568 - cosine_similarity: 0.2522 - perplexity: 98602.3516\n","Epoch 2: val_loss improved from 5.96189 to 5.61907, saving model to /content/drive/MyDrive/PredictNextWords/model_checkpoints/model.h5\n","1162/1162 [==============================] - 1819s 2s/step - loss: 5.7941 - accuracy: 0.1236 - top_k_accuracy: 0.2568 - cosine_similarity: 0.2522 - perplexity: 98602.3516 - val_loss: 5.6191 - val_accuracy: 0.1427 - val_top_k_accuracy: 0.2851 - val_cosine_similarity: 0.2769 - val_perplexity: 228503.8438\n","Epoch 3/50\n","1162/1162 [==============================] - ETA: 0s - loss: 5.5639 - accuracy: 0.1404 - top_k_accuracy: 0.2866 - cosine_similarity: 0.2809 - perplexity: 60308.2969\n","Epoch 3: val_loss improved from 5.61907 to 5.50046, saving model to /content/drive/MyDrive/PredictNextWords/model_checkpoints/model.h5\n","1162/1162 [==============================] - 1848s 2s/step - loss: 5.5639 - accuracy: 0.1404 - top_k_accuracy: 0.2866 - cosine_similarity: 0.2809 - perplexity: 60308.2969 - val_loss: 5.5005 - val_accuracy: 0.1510 - val_top_k_accuracy: 0.3040 - val_cosine_similarity: 0.2919 - val_perplexity: 434448.3438\n","Epoch 4/50\n","1162/1162 [==============================] - ETA: 0s - loss: 5.4433 - accuracy: 0.1472 - top_k_accuracy: 0.2987 - cosine_similarity: 0.2872 - perplexity: 25160.4238\n","Epoch 4: val_loss improved from 5.50046 to 5.43415, saving model to /content/drive/MyDrive/PredictNextWords/model_checkpoints/model.h5\n","1162/1162 [==============================] - 1972s 2s/step - loss: 5.4433 - accuracy: 0.1472 - top_k_accuracy: 0.2987 - cosine_similarity: 0.2872 - perplexity: 25160.4238 - val_loss: 5.4342 - val_accuracy: 0.1562 - val_top_k_accuracy: 0.3130 - val_cosine_similarity: 0.2876 - val_perplexity: 779230.5625\n","Epoch 5/50\n"," 937/1162 [=======================\u003e......] - ETA: 5:35 - loss: 5.3604 - accuracy: 0.1512 - top_k_accuracy: 0.3059 - cosine_similarity: 0.2890 - perplexity: 17099.0020"]}],"source":["def run_everything():\n","    from pickle import load\n","    from keras.models import load_model\n","    import os\n","\n","    SEQUENCE_LENGTH = 100\n","    EPOCHS = 50\n","    BATCH_SIZE = 512\n","    EMBEDDING_DIM = 50\n","    LR = 0.002\n","\n","    FILE_NAME = 'master.txt'\n","    ROOT_DIR = '/content/drive/MyDrive/PredictNextWords'\n","\n","    # # merge all the data\n","    # merge_txt_files(input_dir='./database/hp_unprocessed',\n","    #                 output_dir='database/merged/', name='hp_unprocessed_merged.txt')\n","\n","    # read the data\n","    text = read_txt_file(file_name=FILE_NAME,\n","                         folder_path=ROOT_DIR + \"/database/merged\")\n","\n","    # preprocess the data\n","    preprocessed_tokens = data_preprocessing(text, save_file_name=f'{FILE_NAME}_processed.txt', root_dir=ROOT_DIR, verbose=True, remove_stopwords=False,\n","                                             lemmatize=True,  to_remove_emojis=False, to_correct_typos=False)\n","\n","    del text  # free up memory\n","\n","    # create the word to index mappings\n","    word_index, _, text_tokenized = get_word_to_index_mappings(preprocessed_tokens, root_dir = ROOT_DIR)\n","\n","    del preprocessed_tokens  # free up memory\n","\n","    # get the features and targets\n","    features, targets = get_features_targets(text_tokenized, total_unique_tokens=len(\n","        word_index)+1, seq_len=SEQUENCE_LENGTH, one_hot_encoding=False)\n","\n","    # # save features, targets in df file\n","    # import pandas as pd\n","    # df = pd.DataFrame({'features': features, 'targets': targets})\n","    # df.to_csv('features_targets.csv', index=False)\n","    # del df\n","\n","    # split the data into training, testing and validation sets\n","    X_train, X_test, X_val, y_train, y_test, y_val = split_training_testing_validation(\n","        features, targets, test_size=0.1, val_size=0.1)\n","\n","    embedding_matrix = load_glove(word_index, embedding_dim=EMBEDDING_DIM, root_dir = ROOT_DIR)\n","\n","\n","    print(\n","        f\"\\nSEQUENCE_LENGTH: {SEQUENCE_LENGTH}, EPOCHS: {EPOCHS}, BATCH_SIZE: {BATCH_SIZE}, EMBEDDING_DIM: {EMBEDDING_DIM}, LR: {LR}\")\n","\n","    seq2seq_with_attention(word_index, embedding_matrix=embedding_matrix, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, embedding_dim=EMBEDDING_DIM,\n","                                            seq_len=SEQUENCE_LENGTH, X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val, X_test=X_test, y_test=y_test, root_dir=ROOT_DIR)\n","\n","if __name__ == '__main__':\n","    run_everything()\n"]},{"cell_type":"markdown","metadata":{"id":"7BjPGRo2xbKn"},"source":["## Discussion"]},{"cell_type":"markdown","metadata":{"id":"3eKX-EYWxbKq"},"source":["### Best hyperparameters\n","\n","These results came after trying a lot of different architectures and hyperparameter tuning:\n","* SEQUENCE_LENGTH = 100\n","* EPOCHS = 50 (usually stops around 30 something)\n","* BATCH_SIZE = 512\n","* EMBEDDING_DIM = 50\n","* LEARNING RATE = 0.002\n","* OPTIMIZER = ADAM\n","* DROPOUT RATE = 0.2,\n","* REGULIZER = L2\n","* WEIGHT DECAY = 1e-4\n","\n","The following architecture proved most effective: \u003cbr\u003e\n","![](https://drive.google.com/uc?export=view\u0026id=1fzGpkUeqBEwzvQNt2_S9jYgMsggeJsuN)\n","\n","The output of dense layer is of shape (,Vocabulary size)\n","\n","---\n","\n","## Results\n","\n","**LOSS**: Looking at the training and validation **loss** plots through the epochs we see that they follow the expected pattern: validation loss starts higher than the training loss and they both reduce in a power-law pattern and then they finally converge.\n","\n","**ACCURACY**: Similarly, in the case of **accuracy**, they both start from 0 and grow at the same rate until finally converging at around 15% where they remain for around 10 epochs before being stopped by the EarlyStopping.\n","\n","**TOP K=5 ACCURACY**: As expected, the **top k=5 accuracy** follow the exact same pattern but only being higher on the $y$ axis. They both converge at around 30-35% which is expected since we label more of the predictions as correct.\n","\n","**COSINE SIMILARITY:** As expected, the cosine similarity start from 0 indicating that the vector representation of the predictions and the vector representation of the target are almost completely perpendicular and thus not similar. However, as training pursue, we see an increase in the cosine similarity that follow the pattern seen in *accuracy* and *top k=5 accuracy* until finally converging at aroun 0.28. This is good since the cosine similarity value of 1 would indicate that there is a complete match between the vectors.\n","\n","**PERPLEXITY**: In the case of perplexity, the situation is more tricky. The training perplexity reduces but the validation perplexity increases drastically (jumps by orders of magnitude). I believe this is because of the underlining structure of how perplexity works and because of our task which is not exactly perfect for what perplexity is doing. More specifically, I believe it is not suitable as an evaluation metric for predicting a single word because it is designed to measure the overall performance of a language model in generating a sequence of words based on a given context. We calculate it by taking the inverse of the geometric mean of the probabilities assigned to *each word* in the sequence by the language model. Essentially, it is a measure of how well the language model is able to predict the *entire sequence of words* given the context, not just a single word.\n","\n","In the case of predicting a single word, there is no context or sequence of words to evaluate the language model against. The model is simply making a prediction based on a single input token. Therefore, calculating perplexity for a single word prediction would not be meaningful because it does not capture the ability of the model to predict future words in the sequence.\n","\n","---\n","\n","## Comparison against the old model\n","\n","Now, after trying both the previous model and the current attention seq2seq model with the current expanded dataset, the seq2seq with attention outperformed the old model in all metrics on the testing dataset:\n","* **Accuracy**: 15.1% against 13.25%\n","* **Top 5 Accuracy**: 30.7% against 24.9%\n","* **Cosine similarity**: 0.286 against 0.256\n","* **Perplexity** could not really be used as a viable metric due to the limitations explained above.\n","\n","This indicates that out new model outperforms the previous model by a somewhat significant amount.\n","\n","---\n","\n","## Future versions\n","\n","In the future versions, I will try to find a virtual machine that will allow me to try a bigger dataset and then I will try to create a transformer model and one of the GPT models (likely GPT-2). I can also try out different lemmatizations (that allow for commas, etc.) so I can get proper sentences. If possible, adding RL would be nice to finetune the models that I get.\n","\n","\n","-----\n","\n","\n","### Ethical considerations\n","\n","In this assignment I used significantly more of my own personal data compared to the previous assignment. However, I carefully chose which data I allowed to be used. I did this in a way so that my personal privacy and other peoples' privacy is not compromised in any way. Since the dataset is relatively small, I am confident I did not make any mistakes here. Furthermore, the dataset is biased towards me, but since the goal of the project is a personal autocomplete, this matches the need exactly. I also used 2 books to expand the vocabulary and the size of my dataset, however, both of these books are copyright free and distributed under the free license of Project Gutenberg.\n","\n","The real ethical concerns come up when we try to scale up the dataset which is required for the state of the art LLMs. For their real power to show, they need much, much, much more data so they resort to copying data from the internet, which tends to include personal and copyrighted data. In the case of ChatGPT there is the whole discussion to be had about the implication of it for the job market, education, electricity use, where we will get the data for future, overreliance, etc."]}],"metadata":{"colab":{"collapsed_sections":["bcs4zJIoxbKP","R0x2iRegxbKh"],"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"924b6ded23d1260d56f01f9496c1fa7915f2e054489eccd3a656c41d58155093"}}},"nbformat":4,"nbformat_minor":0}